{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Python Basics Tutorial\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "####  Machine Learning Mastery with Python\n",
    "####  Jason Brownlee\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this recipe:\n",
    "- Univariate selection\n",
    "- Recursive Feature Elimination\n",
    "- PCA\n",
    "- Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Various statistical tests can be used to test the relationship\n",
    "##   between the dependent and independent variables\n",
    "\n",
    "## this example uses chi-sq\n",
    "\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "path = 'D:\\\\OneDrive - QJA\\\\My Files\\\\DataScience\\\\DataSets'\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', \n",
    "         'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(path + '\\\\' + filename,\n",
    "                    names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "array = dataframe.values\n",
    "\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "# create object to perform SelectKBest using chi-sq test\n",
    "test = SelectKBest(score_func = chi2, k = 4)\n",
    "\n",
    "# test independent variables using test object above\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# print scores for each attribute.  highest scores indicate\n",
    "# biggest influence of features on dependent variable\n",
    "set_printoptions(precision = 3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# prints the features with the highest scores\n",
    "# corresponds to plas, test, mass, age\n",
    "#   (need to map fit.score index to index of attribute names)\n",
    "print(features[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "- Recursively removes attributes to build model on those that remain\n",
    "\n",
    "- Note: like other feature selection algos, use this as a guide, only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features: ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
      "Number of Features: 3\n",
      "SelectedFeatures: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "# create object contianing LR algorithm\n",
    "# use vars(object) to see object attributes\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "# model\n",
    "rfe = RFE(model, 3) # object estimator for RFE model with 3 features\n",
    "# rfe\n",
    "fit = rfe.fit(X, Y) # apply rfe on dep and indep vars\n",
    "\n",
    "print('All Features: %s' % names[:-1]) # -1 exclude dep var\n",
    "print(\"Number of Features: %d\" % fit.n_features_)\n",
    "print('SelectedFeatures: %s' % fit.support_)\n",
    "print('Feature Ranking: %s' % fit.ranking_)\n",
    "\n",
    "# top 3 choices are preg, mass, pedi (indicated by \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "- linear alg to reduce variable dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "# object to contain PCA algo\n",
    "pca = PCA(n_components = 3)\n",
    "fit = pca.fit(X) # store pca of X in fit\n",
    "\n",
    "print('Explained Variance: %s' % fit.explained_variance_ratio_)\n",
    "print(fit.components_)\n",
    "\n",
    "# first component contains approv 89% of variance with dep var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "- uses bagged decision trees like RandForest, Extra Trees\n",
    "\n",
    "- in this example, ExtraTreesClassifier uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features: ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
      "[0.106 0.239 0.099 0.08  0.076 0.142 0.12  0.139]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "# object to contain model estimator algorithm\n",
    "model = ExtraTreesClassifier(n_estimators = 100)\n",
    "model.fit(X, Y) # apply model to data\n",
    "\n",
    "# gives importance score for each attribute\n",
    "print('All Features: %s' % names[:-1]) # -1 exclude dep var\n",
    "print(model.feature_importances_)\n",
    "\n",
    "# plas, age, and mass have highest score (most influential)\n",
    "# see how this compares with other methods used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
