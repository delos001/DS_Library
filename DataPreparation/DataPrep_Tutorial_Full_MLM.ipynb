<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for ML:\n",
    " \n",
    "- From: Jason Brownlee\n",
    "- MML Book: 'Data Preparation for Machine Learning' v1.1, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Overview:\n",
    "## Steps:\n",
    "1. Define problem\n",
    "2. Prepare Data\n",
    "3. Eval Models\n",
    "4. Finalize Model\n",
    "\n",
    "## Reminder:\n",
    "- ML Algorithms require data to be numbers\n",
    "- Some ML Algorithms impost requirements on the data \n",
    "    - specific distribution needed, \n",
    "    - removal of correlative, \n",
    "    - non-preditive vars,\n",
    "    - redundant vars\n",
    "- Statistical noise and errors in data may need to be corrected\n",
    "- Non-linear relationships may need to be identified\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepartion Tasks:\n",
    "- cleaning\n",
    "    - redundant samples and features\n",
    "    - extreme values (outliers)\n",
    "    - mark and impute missing data\n",
    "- feature selection\n",
    "    - unsupervised\n",
    "    - supervised: trees, RFE, statistics\n",
    "    - \n",
    "- transforms\n",
    "    - Numerical Type:\n",
    "        - Change Distribution:\n",
    "            - descretization transform: encode numeric to ordinal\n",
    "            - power: distribution of a variable to more Gausian\n",
    "            - quantile: impose probability distribution such as uniform or Gaussian\n",
    "        - Change Scale:\n",
    "            - normalize: scale variable to range 0-1\n",
    "            - standardization: scale variable to standard Gaussian\n",
    "            - robust\n",
    "        - Engineer:\n",
    "            - polynomial (see feature engineering below)\n",
    "    - Categorical Type:\n",
    "        - Nominal Type:\n",
    "            - one-hot encode: encode categorical to binary\n",
    "            - dummy encode\n",
    "        - Ordinal Type:\n",
    "            - ordinal encode: encode categorical to integer\n",
    "        \n",
    "     \n",
    "- feature engineering\n",
    "    - add boolean flag, group or global summary statistic, create new vars\n",
    "    - polynomial transforms: create copies of numerical input vars that are raised to a power\n",
    "- dimensionality reduction\n",
    "    - Matrix Factorization:\n",
    "        - PCA\n",
    "        - SVD (singular value decomposition)\n",
    "    - Maniforld Learning:\n",
    "        - SOM (self-organizing maps)\n",
    "        - tSNE (t-distributed Stochastic Neighbor Embedding)\n",
    "    - Model Based:\n",
    "        - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation without Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "\n",
    "- leakage occurs when knowledge of hold-out test set leaks into training set because you didn't split them first\n",
    "- data prep must be fit on training dataset only\n",
    "\n",
    "- Correct process:\n",
    "    - split data\n",
    "    - fit data pre on training data\n",
    "    - apply data prep to train and test data\n",
    "    - eval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Example with Naive Data Preparation\n",
    "- INCORRECT way that doesn't split first\n",
    "    - this is to compare results to the correct process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "## define dataset\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=20,\n",
    "                           n_informative=15,\n",
    "                           n_redundant=5,\n",
    "                           random_state=7)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we are scaling before the split creating data leakageabs\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "## split scaled data inot train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.848\n"
     ]
    }
   ],
   "source": [
    "## fit logistic model on train set\n",
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(X_train, y_train)\n",
    "\n",
    "## evaluate model using test set\n",
    "yhatLR = modelLR.predict(X_test)\n",
    "accuracyLR = accuracy_score(y_test, yhatLR)\n",
    "\n",
    "print('Accuracy: %.3f' % (accuracyLR*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Example with Correct Data Preparation\n",
    "- split first, then apply data prep to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.33,\n",
    "                                                   random_state=1)\n",
    "\n",
    "\n",
    "## define scaler and fit on training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "## scale training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#print(X_train[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.455\n"
     ]
    }
   ],
   "source": [
    "modelLR2 = LogisticRegression()\n",
    "modelLR2.fit(X_train, y_train)\n",
    "\n",
    "## evaluate model against test set\n",
    "yhatLR2 = modelLR2.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, yhatLR2)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation Data Preparation\n",
    "\n",
    "- in this example, used on synthetic binary classification\n",
    "- k-fold algorithm: split data into k non-overlapping groups of rows\n",
    "    - model trained on all but one group to form training dataset\n",
    "    - repeated so each fold is given opp to be the hold-out set\n",
    "    - average performace across all runs is used \n",
    "- generally more reliable than train-test split but also computationally expensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Example with Naive Preparation:\n",
    "- INCORRECT method would be to apply data prep algorithm prior \n",
    "- involves apply data transforms first and then using the cross-validation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.300 (3.607)\n"
     ]
    }
   ],
   "source": [
    "## create sample data\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=20,\n",
    "                           n_informative=15,\n",
    "                           n_redundant=5,\n",
    "                           random_state=7)\n",
    "\n",
    "\n",
    "\n",
    "## standarize the data, fit on X, then transform on X\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X) #first fit, then transform (combines 2 steps)\n",
    "\n",
    "## define model\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "## define eval procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,\n",
    "                             random_state=1)\n",
    "\n",
    "scores = cross_val_score(modelLR, X, y,\n",
    "                         scoring='accuracy',\n",
    "                         cv=cv,\n",
    "                         n_jobs=1)\n",
    "\n",
    "## report accuracy\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Example with Correct Data Preparation:\n",
    "- Requires that data prep method is prepared on training and test sets within the cv procedure\n",
    "    - Requires pipeline to do this for each iteration of the procedure\n",
    "    - This can be achieved using the Pipeline class. \n",
    "        - This class takes a list of steps that define the pipeline. \n",
    "        - Each step in the list is a tuple with two elements. \n",
    "            - the first element is the name of the step (a string) and \n",
    "            - the second is the configured object of the step, such as a transform or a model. \n",
    "        - The model is only supported as the final step, \n",
    "            - although we can have as many transforms as we like in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.433 (3.471)\n"
     ]
    }
   ],
   "source": [
    "## create sample data\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=20,\n",
    "                           n_informative=15,\n",
    "                           n_redundant=5,\n",
    "                           random_state=7)\n",
    "\n",
    "## define pipeline\n",
    "steps = list()  # see description of tuples in list, above\n",
    "steps.append(('scalar', MinMaxScaler()))\n",
    "steps.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "## define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,\n",
    "                             random_state=1)\n",
    "\n",
    "## evaluate model using cross validation\n",
    "scores = cross_val_score(pipeline, X, y, \n",
    "                         scoring='accuracy',\n",
    "                         cv=cv,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "## print accuracy results\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "- Messy Data\n",
    "    - example data sets to play with: \n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.names\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.names\n",
    "- ID and Delete columns containing single variable\n",
    "- Consider columns with few variables\n",
    "- Remove columns with low variance\n",
    "- Identify and Delete rows containing duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Single Variable Columns\n",
    "- ie: low or zero variance, which have no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "data = loadtxt(url, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 238\n",
      "1 297\n",
      "2 927\n",
      "3 933\n",
      "4 179\n",
      "5 375\n",
      "6 820\n",
      "7 618\n",
      "8 561\n",
      "9 57\n",
      "10 577\n",
      "11 59\n",
      "12 73\n",
      "13 107\n",
      "14 53\n",
      "15 91\n",
      "16 893\n",
      "17 810\n",
      "18 170\n",
      "19 53\n",
      "20 68\n",
      "21 9\n",
      "22 1\n",
      "23 92\n",
      "24 9\n",
      "25 8\n",
      "26 9\n",
      "27 308\n",
      "28 447\n",
      "29 392\n",
      "30 107\n",
      "31 42\n",
      "32 4\n",
      "33 45\n",
      "34 141\n",
      "35 110\n",
      "36 3\n",
      "37 758\n",
      "38 9\n",
      "39 9\n",
      "40 388\n",
      "41 220\n",
      "42 644\n",
      "43 649\n",
      "44 499\n",
      "45 2\n",
      "46 937\n",
      "47 169\n",
      "48 286\n",
      "49 2\n"
     ]
    }
   ],
   "source": [
    "## summarize number of unique values in each column OPTION 1\n",
    "for i in range(data.shape[1]):\n",
    "    print(i, len(unique(data[:, i])))\n",
    "    \n",
    "## here we can look for columsn that have 1 or low number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     238\n",
      "1     297\n",
      "2     927\n",
      "3     933\n",
      "4     179\n",
      "5     375\n",
      "6     820\n",
      "7     618\n",
      "8     561\n",
      "9      57\n",
      "10    577\n",
      "11     59\n",
      "12     73\n",
      "13    107\n",
      "14     53\n",
      "15     91\n",
      "16    893\n",
      "17    810\n",
      "18    170\n",
      "19     53\n",
      "20     68\n",
      "21      9\n",
      "22      1\n",
      "23     92\n",
      "24      9\n",
      "25      8\n",
      "26      9\n",
      "27    308\n",
      "28    447\n",
      "29    392\n",
      "30    107\n",
      "31     42\n",
      "32      4\n",
      "33     45\n",
      "34    141\n",
      "35    110\n",
      "36      3\n",
      "37    758\n",
      "38      9\n",
      "39      9\n",
      "40    388\n",
      "41    220\n",
      "42    644\n",
      "43    649\n",
      "44    499\n",
      "45      2\n",
      "46    937\n",
      "47    169\n",
      "48    286\n",
      "49      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## summarize number of unique values in each column OPTION 2\n",
    "## this method uses pandas nunique function (so read_csv to read in as df)\n",
    "data1 = read_csv(url, header=None)\n",
    "print(data1.nunique())\n",
    "## here we can look for columsn that have 1 or low number of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Single Var Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[22]\n",
      "(937, 49)\n"
     ]
    }
   ],
   "source": [
    "data1 = read_csv(url, header=None)\n",
    "print(data1.shape)\n",
    "\n",
    "## get number of unique values for each col\n",
    "unqCnt = data1.nunique()\n",
    "\n",
    "## identify columns (by index number) to delete (those cols with only 1 value)\n",
    "## enumerate loops through list and adds a index/counter\n",
    "## i is the index/counter and v is the nunique value \n",
    "toDel = [i for i, v in enumerate(unqCnt) if v==1]\n",
    "print(toDel)\n",
    "\n",
    "## delete columns with only 1 valua\n",
    "data1.drop(toDel, axis=1, inplace=True)\n",
    "print(data1.shape)\n",
    "## in this example, col 22 had only one value: therefor column count goes from 50 to 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Columns with Few Values\n",
    "- it may make sense for categorical variables to have few values but less so for numerical columns\n",
    "    - these may have little to no variance and therefore, less predictive of dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 238, 25.4%\n",
      "1, 297, 31.7%\n",
      "2, 927, 98.9%\n",
      "3, 933, 99.6%\n",
      "4, 179, 19.1%\n",
      "5, 375, 40.0%\n",
      "6, 820, 87.5%\n",
      "7, 618, 66.0%\n",
      "8, 561, 59.9%\n",
      "9, 57, 6.1%\n",
      "10, 577, 61.6%\n",
      "11, 59, 6.3%\n",
      "12, 73, 7.8%\n",
      "13, 107, 11.4%\n",
      "14, 53, 5.7%\n",
      "15, 91, 9.7%\n",
      "16, 893, 95.3%\n",
      "17, 810, 86.4%\n",
      "18, 170, 18.1%\n",
      "19, 53, 5.7%\n",
      "20, 68, 7.3%\n",
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "23, 92, 9.8%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "27, 308, 32.9%\n",
      "28, 447, 47.7%\n",
      "29, 392, 41.8%\n",
      "30, 107, 11.4%\n",
      "31, 42, 4.5%\n",
      "32, 4, 0.4%\n",
      "33, 45, 4.8%\n",
      "34, 141, 15.0%\n",
      "35, 110, 11.7%\n",
      "36, 3, 0.3%\n",
      "37, 758, 80.9%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "40, 388, 41.4%\n",
      "41, 220, 23.5%\n",
      "42, 644, 68.7%\n",
      "43, 649, 69.3%\n",
      "44, 499, 53.3%\n",
      "45, 2, 0.2%\n",
      "46, 937, 100.0%\n",
      "47, 169, 18.0%\n",
      "48, 286, 30.5%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "## here we summarize the percentage of unique values for each column\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "\n",
    "## define data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "## load data\n",
    "data2 = loadtxt(url, delimiter=',')\n",
    "\n",
    "## summarize unique values for each column\n",
    "for i in range(data2.shape[1]):\n",
    "    num = len(unique(data2[:, i]))\n",
    "    perc = float(num) / data2.shape[0] * 100\n",
    "    print('%d, %d, %.1f%%' % (i, num, perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "32, 4, 0.4%\n",
      "36, 3, 0.3%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "45, 2, 0.2%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "## Identify only those variables that are < 1%\n",
    "## once identified, these need to be further analyzed before deciding to delete or not\n",
    "for i in range(data2.shape[1]):\n",
    "    num = len(unique(data2[:, i]))\n",
    "    perc = float(num) / data.shape[0] * 100\n",
    "    if perc < 1:\n",
    "        print('%d, %d, %.1f%%' % (i, num, perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]\n",
      "(937, 39)\n"
     ]
    }
   ],
   "source": [
    "## In this example, we chose to delete all 11 columns identified above \n",
    "\n",
    "data3 = read_csv(url, header=None)\n",
    "## get unique counts for each column \n",
    "counts = data3.nunique()\n",
    "\n",
    "## identify columns to delete \n",
    "toDel2 = [i for i, v in enumerate(counts) if (float(v)/data3.shape[0] * 100) < 1]\n",
    "## print column numbers to delete \n",
    "print(toDel2)\n",
    "\n",
    "data3.drop(toDel2, axis=1, inplace=True)\n",
    "print(data3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Columns with Low Variance\n",
    "- another (probably better approach) is to consider variance of the column\n",
    "- variance = av squared difference of values in sample, from the mean\n",
    "    - column with single value has variance of 0\n",
    "    \n",
    "- VarianceThreshold class from scikit-learn supports this as a type of feature selection\n",
    "    - default is 0.0 but we can specify a threshold, then fit and apply using fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 49) (937,)\n",
      "(937, 48)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "## load data\n",
    "data4 = read_csv(url, header=None)\n",
    "\n",
    "## split data into input and output \n",
    "data4 = data4.values\n",
    "X = data4[:, :-1]\n",
    "y = data4[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## define transform\n",
    "transf = VarianceThreshold()\n",
    "\n",
    "## transform input data \n",
    "## because we don't specify threshold above, uses 0.0 default %& removes cols with 0.0 variance\n",
    "X_sel = transf.fit_transform(X)\n",
    "print(X_sel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Threshold=0.00, Features=48\n",
      ">Threshold=0.05, Features=37\n",
      ">Threshold=0.10, Features=36\n",
      ">Threshold=0.15, Features=35\n",
      ">Threshold=0.20, Features=35\n",
      ">Threshold=0.25, Features=35\n",
      ">Threshold=0.30, Features=35\n",
      ">Threshold=0.35, Features=35\n",
      ">Threshold=0.40, Features=35\n",
      ">Threshold=0.45, Features=33\n",
      ">Threshold=0.50, Features=31\n"
     ]
    }
   ],
   "source": [
    "## repeat above but specify thresholds that are placed in a list \n",
    "from numpy import arange\n",
    "from matplotlib import pyplot\n",
    "## define thresholds to test \n",
    "thresholds = arange(0.0, 0.55, 0.05)\n",
    "\n",
    "## apply transform with each threshold \n",
    "results = list()\n",
    "for t in thresholds:\n",
    "    ## define the transform \n",
    "    transf = VarianceThreshold(threshold=t)\n",
    "    ## transform input data \n",
    "    X_sel = transf.fit_transform(X)\n",
    "    ## determine how many input features there are for each threshold \n",
    "    n_feats = X_sel.shape[1]\n",
    "    print('>Threshold=%.2f, Features=%d' % (t, n_feats))\n",
    "    results.append(n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot relationship between threshold and results \n",
    "##   notice steep drops: 0.0-0.05 and 0.4-0.5 \n",
    "pyplot.plot(thresholds, results)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Duplicate Data\n",
    "- using train/test/split or k-fold cv could result in same row being in the train and eval of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "irisURL = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "       0    1    2    3               4\n",
      "34   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "37   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "irisData = read_csv(irisURL, header=None)\n",
    "\n",
    "## identify duplicates \n",
    "dups = irisData.duplicated()\n",
    "print(dups.any())  ## True/False whether there are any dups\n",
    "print(irisData[dups]) ## print rows containing the duplicates\n",
    "## note 3 duplicated rows are identified in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Rows with Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "print(irisData.shape)\n",
    "irisData.drop_duplicates(inplace=True)\n",
    "print(irisData.shape)\n",
    "## note: 3 dup rows were identified and it dropped 3 (150-147)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Identification and Removal\n",
    "- very simply, outliers are data that fall outside the expected range of a dataset\n",
    "    - unlikely, rare, distinct (measurement or input error, data corruption, true outlier)\n",
    "    - can have many causes\n",
    "    \n",
    "- In this tutorial:\n",
    "    - use of simple univariate statistics (stdev, IQR)\n",
    "    - use of automatic outlier detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gaussian Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 50.049, stdv = 4.994\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn  ## generates gaussian dist\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "seed(1)\n",
    "\n",
    "## Define tst data: \n",
    "    ## 10,000 random numbers \n",
    "    ## drawn from Gaussian distribution\n",
    "    ## mean 50, std 5\n",
    "data5 = 5 * randn(10000) + 50\n",
    "## confirm distribution specs\n",
    "print('mean = %.3f, stdv = %.3f' % (mean(data5), std(data5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation Method\n",
    "- stdev can be used if we know distribution is at least Gaussian-like\n",
    "    - within 1 stdev, we expect to see ~68% of the data\n",
    "    - within 2 stdev, we expect to see ~95% of the data\n",
    "    - within 3 stdev, we expect to see ~99.7% of the data\n",
    "- 3 stdev from mean is common cut-off for identifying outliers in Gaussian-like dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified outliers: 29\n",
      "Non-outliers: 9971\n"
     ]
    }
   ],
   "source": [
    "## calculate summary stats\n",
    "data_mean, data_std = mean(data5), std(data5)\n",
    "\n",
    "## define outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "\n",
    "## identify outliers in the dataset\n",
    "outliers = [x for x in data5 if x < lower or x > upper]\n",
    "outliersIdx = [i for i, x in enumerate(data5) if x < lower or x > upper]\n",
    "#print(outliers); print(outliersIdx)\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "## remove outliers\n",
    "outliersRem = [x for x in data5 if x > lower and x < upper]\n",
    "print('Non-outliers: %d' % len(outliersRem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interquartile Range Method\n",
    "- data is often not Gaussian: IQR is good method for summarizing non-gaussian distributions\n",
    "    - recall the 50th percentile is middle value (or average of 2 middle values)\n",
    "    - IQR is the middle 50 percent of the data\n",
    "\n",
    "- Outlier factors:\n",
    "    - common outlier factor is 1.5 above 75th or below 25th\n",
    "    - factors of 3*IQR above 75th or below 25th are extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn  ## generates gaussian dist\n",
    "from numpy import percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles: 25th = 46.685, 75 = 53.359, IQR = 6.674\n",
      "Identified outliers: 0\n",
      "Non-outliers: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed(1)\n",
    "\n",
    "## define data\n",
    "data6 = 5 * randn(10000) + 50\n",
    "\n",
    "## define outlier factor (normal, extreme)\n",
    "z = 3\n",
    "\n",
    "## calculate IQR\n",
    "q25, q75 = percentile(data5, 25), percentile(data5, 75)\n",
    "iqr = q75-q25\n",
    "print('Percentiles: 25th = %.3f, 75 = %.3f, IQR = %.3f' % (q25, q75, iqr))\n",
    "\n",
    "## calculate cutoffs\n",
    "cut_off = iqr*z\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "outliers = [x for x in data6 if x < lower or x > upper]\n",
    "#print(outliers)\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "## remove outliers\n",
    "outliersRem = [x for x in data6 if x > lower and x < upper]\n",
    "print('Non-outliers: %d' % len(outliersRem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Outlier Detection\n",
    "\n",
    "- One-class classification\n",
    "    - 'aims at capturing characteristics of training instances, in order to be able to distinguish between them and potential outliers to appear.\n",
    "    - locate examples that are far from other examples in multidimensional space\n",
    "        - can become less reliable as number of features increases\n",
    "    - Local outlier factor:\n",
    "        - uses idea of nearest neighbor\n",
    "        - each observation assigned score related to how isolated or how likely it is to be outlier based on size of its local neighborhood\n",
    "            - larges score are more likely to be outliers\n",
    "            \n",
    "- **Note: this example uses LocalOutlierFactor but scikitlearn also offers IsolationForest algorithm which can be used in same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "(339, 13) (167, 13) (339,) (167,)\n"
     ]
    }
   ],
   "source": [
    "## Without Outliers Removed (to be later compared to model with outliers removed)\n",
    "## define data location\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "\n",
    "data7 = read_csv(url, header=None)\n",
    "\n",
    "## create array\n",
    "data7 = data7.values\n",
    "\n",
    "## split into input and output\n",
    "X, y = data7[:, :-1], data7[:, -1]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## split into train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 3.417\n"
     ]
    }
   ],
   "source": [
    "## generate linear regression model\n",
    "\n",
    "## fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "## evaluate model\n",
    "modelPred = model.predict(X_test)\n",
    "\n",
    "## evaluate error\n",
    "mae = mean_absolute_error(y_test, modelPred)\n",
    "print('Mean Absolute Error %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "(339, 13) (339,)\n"
     ]
    }
   ],
   "source": [
    "## With Outliers Removed\n",
    "\n",
    "## read data in and convert to array\n",
    "data8 = read_csv(url, header=None)\n",
    "data8 = data8.values\n",
    "\n",
    "## split into input and output\n",
    "X, y = data7[:, :-1], data7[:, -1]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## split into train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305, 13) (305,)\n",
      "Mean Absolute Error 3.356\n"
     ]
    }
   ],
   "source": [
    "## identify outliers in training data set\n",
    "## define method  (normal data marked with 1, outlier marked with -1)\n",
    "lof = LocalOutlierFactor()  ## uses default (see documentation to tune)\n",
    "X_trainLOF = lof.fit_predict(X_train) ## creates array of 1 or -1\n",
    "#print(X_trainLOF)\n",
    "\n",
    "## define variable to select rows that aren't outliers\n",
    "tag = X_trainLOF != -1  ## creates array of true/false\n",
    "#print(tag)\n",
    "\n",
    "## create new X and y train: observations where tag = True\n",
    "X_trainRem, y_trainRem = X_train[tag, :], y_train[tag]\n",
    "print(X_trainRem.shape, y_trainRem.shape)  ## new shape confirms outlier removal\n",
    "\n",
    "## define and fit the model\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_trainRem, y_trainRem)\n",
    "\n",
    "## predict on test set\n",
    "model2Pred = model2.predict(X_test)\n",
    "## evaluate error\n",
    "mae2 = mean_absolute_error(y_test, model2Pred)\n",
    "print('Mean Absolute Error %.3f' % mae2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "- note that some missing values may be indicated in odd ways or ways that don't make sense\n",
    "    - ex: a zero for BMI would indicate missing since no one can have zero body mass\n",
    "    - ex: a negative number in numeric field that can only be positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import arange\n",
    "## load data  (note: see section 5 for data set 'names' link)\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "     0    1   2   3    4     5      6   7  8\n",
      "0    6  148  72  35    0  33.6  0.627  50  1\n",
      "1    1   85  66  29    0  26.6  0.351  31  0\n",
      "2    8  183  64   0    0  23.3  0.672  32  1\n",
      "3    1   89  66  23   94  28.1  0.167  21  0\n",
      "4    0  137  40  35  168  43.1  2.288  33  1\n",
      "5    5  116  74   0    0  25.6  0.201  30  0\n",
      "6    3   78  50  32   88  31.0  0.248  26  1\n",
      "7   10  115   0   0    0  35.3  0.134  29  0\n",
      "8    2  197  70  45  543  30.5  0.158  53  1\n",
      "9    8  125  96   0    0   0.0  0.232  54  1\n",
      "10   4  110  92   0    0  37.6  0.191  30  0\n",
      "11  10  168  74   0    0  38.0  0.537  34  1\n",
      "12  10  139  80   0    0  27.1  1.441  57  0\n",
      "13   1  189  60  23  846  30.1  0.398  59  1\n",
      "14   5  166  72  19  175  25.8  0.587  51  1\n",
      "15   7  100   0   0    0  30.0  0.484  32  1\n",
      "16   0  118  84  47  230  45.8  0.551  31  1\n",
      "17   7  107  74   0    0  29.6  0.254  31  1\n",
      "18   1  103  30  38   83  43.3  0.183  33  0\n",
      "19   1  115  70  30   96  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "data9 = read_csv(url, header=None)\n",
    "print(data9.describe())\n",
    "print(data9.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    111\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8    500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## get count of missing numbers for each variable\n",
    "## zeros for some columns expected: need to review data documentation to figure out which ones\n",
    "missingCnt = (data9[arange(0,data9.shape[1])] == 0).sum()\n",
    "print(missingCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "\n",
    "data10 = read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n",
      "     0      1     2     3      4     5      6   7  8\n",
      "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
      "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
      "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
      "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
      "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
      "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
      "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
      "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
      "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
      "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
      "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
      "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
      "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
      "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
      "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
      "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
      "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
      "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
      "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
      "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "## replace zeros with nan for specified columns\n",
    "##   review of columns indicates zeros aren't sensible for columns 1, 2, 3, 4, 5 \n",
    "nanCols = [1, 2, 3, 4, 5]\n",
    "data10[nanCols] = data10[nanCols].replace(0, nan)\n",
    "\n",
    "## alternative syntax\n",
    "#data10[[1, 2, 3, 4, 5]] = data10[[1, 2, 3, 4,5]].replace(0, nan)\n",
    "\n",
    "print(data10.isnull().sum())\n",
    "print(data10.head(20))  ## check to make sure NaN added correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values impact to Algorithms\n",
    "- some machine learning algorithms cannot handle missing or nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 425, in fit\n",
      "    dtype=[np.float64, np.float32])\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 803, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 646, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 100, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 425, in fit\n",
      "    dtype=[np.float64, np.float32])\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 803, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 646, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 100, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 425, in fit\n",
      "    dtype=[np.float64, np.float32])\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 803, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 646, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 100, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "#### NOTE: THIS WILL FAIL BECAUSE OF PRESENCE OF NAN\n",
    "\n",
    "## load data\n",
    "data11 = read_csv(url, header=None)\n",
    "\n",
    "## replace zeros with nan\n",
    "nanCol = [1, 2, 3, 4, 5]\n",
    "data11[nanCol] = data11[nanCol].replace(0, nan)\n",
    "\n",
    "## convert to array\n",
    "data11 = data11.values\n",
    "## split data into input and output\n",
    "X, y = data11[:, :-1], data11[:, -1]\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "## define model and model evaluation (cv) procedure\n",
    "model = LinearDiscriminantAnalysis()\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "## evaluate model\n",
    "modelEval = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "print('Accuracy: %.3f' % modelEval.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Missing Values\n",
    "- simplest strategy to handle missing values is to remove those associated observations\n",
    "    - BUT BE CAREFUL because this could have negative effects\n",
    "    - sometimes that a value is missing can be predictive\n",
    "    - there are other strategies to impute missing values so as to not lose information via removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "Accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "## In this example, we remove rows that have nan so the LDA can be performed\n",
    "data12 = read_csv(url, header=None)\n",
    "print(data12.shape)\n",
    "\n",
    "nanCols = [1, 2, 3, 4, 5]\n",
    "data12[nanCols] = data12[nanCols].replace(0, nan)\n",
    "## drop rows with missing values (nan's)\n",
    "data12.dropna(inplace=True)\n",
    "\n",
    "## we can now run LDA on the reduce dataset\n",
    "data12 = data12.values  ## create array\n",
    "X, y = data12[:, :-1], data12[:, -1]\n",
    "\n",
    "## define model and evaluation procedure\n",
    "model = LinearDiscriminantAnalysis()\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "## evaluate model\n",
    "modelEval = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "print('Accuracy: %.3f' % modelEval.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Imputation\n",
    "- replacing missing values with a statistical replacement:\n",
    "    - is relatively simple (common types: mean, median, mode, constant)\n",
    "    - allows you to keep the observation (rather than delete potentially useful rows)\n",
    "    - often results in good performance\n",
    "    \n",
    "- In this tutorial:\n",
    "    - statistical imputation\n",
    "    - SimpleImputer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer Class: Simple Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import isnan\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28)\n",
      "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
      "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
      "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
      "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
      "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
      "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
      "\n",
      "    20   21   22  23     24  25  26  27  \n",
      "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
      "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
      "2  NaN  NaN  1.0   2      0   0   0   1  \n",
      "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
      "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "data13 = read_csv(url, header=None, na_values='?')\n",
    "print(data13.shape)\n",
    "print(data13.head(5))  ## notice the question marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 1 (0.33%)\n",
      "> 1, Missing: 0 (0.00%)\n",
      "> 2, Missing: 0 (0.00%)\n",
      "> 3, Missing: 60 (20.00%)\n",
      "> 4, Missing: 24 (8.00%)\n",
      "> 5, Missing: 58 (19.33%)\n",
      "> 6, Missing: 56 (18.67%)\n",
      "> 7, Missing: 69 (23.00%)\n",
      "> 8, Missing: 47 (15.67%)\n",
      "> 9, Missing: 32 (10.67%)\n",
      "> 10, Missing: 55 (18.33%)\n",
      "> 11, Missing: 44 (14.67%)\n",
      "> 12, Missing: 56 (18.67%)\n",
      "> 13, Missing: 104 (34.67%)\n",
      "> 14, Missing: 106 (35.33%)\n",
      "> 15, Missing: 247 (82.33%)\n",
      "> 16, Missing: 102 (34.00%)\n",
      "> 17, Missing: 118 (39.33%)\n",
      "> 18, Missing: 29 (9.67%)\n",
      "> 19, Missing: 33 (11.00%)\n",
      "> 20, Missing: 165 (55.00%)\n",
      "> 21, Missing: 198 (66.00%)\n",
      "> 22, Missing: 1 (0.33%)\n",
      "> 23, Missing: 0 (0.00%)\n",
      "> 24, Missing: 0 (0.00%)\n",
      "> 25, Missing: 0 (0.00%)\n",
      "> 26, Missing: 0 (0.00%)\n",
      "> 27, Missing: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "## summarize missing data\n",
    "for i in range(data13.shape[1]):\n",
    "    n_miss = data13[[i]].isnull().sum()\n",
    "    per_miss = n_miss / data13.shape[0] * 100\n",
    "    print('> %d, Missing: %d (%.2f%%)' % (i, n_miss, per_miss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n"
     ]
    }
   ],
   "source": [
    "## split into dep and independ var\n",
    "\n",
    "data = data13.values ## convert to array\n",
    "\n",
    "## get index for dependent variable columns (cols 1-22 )\n",
    "dataX = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, dataX], data[:, 23]\n",
    "\n",
    "## summarize total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "## define imputer and fit on data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X)\n",
    "\n",
    "## transform data\n",
    "Ximp = imputer.transform(X)\n",
    "print('Missing: %d' % sum(isnan(Ximp).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer with Model Evaluation\n",
    "- in this example, k-fold cv is used\n",
    "    - to avoid data leakage, the imputer should be applied to each fold (not entire dataset)\n",
    "    - this is done using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import nan\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "data14 = read_csv(url, header=None, na_values='?')\n",
    "data = data14.values\n",
    "\n",
    "## split data into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.866 (0.061)\n"
     ]
    }
   ],
   "source": [
    "## define model pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "## define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,\n",
    "                             n_repeats=3,\n",
    "                             random_state=1)\n",
    "\n",
    "## evaluate the model\n",
    "dataModel = cross_val_score(pipeline, X, y,\n",
    "                            scoring='accuracy',\n",
    "                            cv=cv,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(dataModel), std(dataModel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer: Compare Different Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "data15 = read_csv(url, header=None, na_values='?')\n",
    "\n",
    "data = data15.values\n",
    "\n",
    "## get index for the independent variables\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "## split into dep and indep variables\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">mean 0.861, (0.062)\n",
      ">median 0.873, (0.055)\n",
      ">most_frequent 0.868, (0.051)\n",
      ">constant 0.879, (0.052)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVpklEQVR4nO3df5Bd5X3f8feHBQIGAsio1EaAaKIYCcVQ2KGOTWz8C0Nag03SFurGMZWrYQrE48auGcszdcvQcYqbjl1oNDRWGTxYZOpaGGcYsIcfppAw1sroJ4ZaI0hQ5YZVobgxIejHt3/cI3NZdrVX0l3u3bPv18yZvec8zzn3uY+OPvfZ5+y9J1WFJKm9Dht0AyRJM8ugl6SWM+glqeUMeklqOYNeklru8EE3YDInnXRSLVy4cNDNkKRZY926dTurav5kZUMZ9AsXLmRsbGzQzZCkWSPJn09V5tSNJLWcQS9JLWfQS1LLGfSS1HIGvSS13LRBn2RVkueSbJ6iPEm+mmRrko1Jzu0quzjJU03Z9f1suCSpN72M6G8DLt5P+SXAomZZDvwhQJIR4JamfAlwZZIlh9JYSdKBmzboq+ph4Pn9VLkMuL06HgNOSPIW4Hxga1Vtq6pXgDubupKkN1A/PjB1CvBs1/r2Zttk2//eVAdJspzObwScdtppfWiWJO1fkr4eb1jv79GPi7GT9VTtZ/ukqurWqhqtqtH58yf9FK8k9VVV9bT0WndY9WNEvx04tWt9AbADOHKK7ZKkN1A/RvR3Ax9v/vrmHcCLVfUTYC2wKMkZSY4ErmjqSpLeQNOO6JOsBi4ETkqyHfjXwBEAVbUSuAf4DWAr8BJwVVO2O8m1wH3ACLCqqrbMwGuQJO3HtEFfVVdOU17ANVOU3UPnjUCSNCB+MlaSWs6gl6SWM+glqeUMeklqOYNeklrOoJekljPoJanlDHpJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklqOYNeklrOoJekljPoJanlpr1n7FyUpK/H69xWd+6yP/vHvtTBMOgn0cvJn8T/JD2yP/un1z6yP9XNqRtJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SW6ynok1yc5KkkW5NcP0n5iUnWJNmY5AdJlnaVPZNkU5L1Scb62XhJ0vSm/cBUkhHgFuCDwHZgbZK7q+qJrmqfB9ZX1UeTnNnUf39X+Xuramcf2y1J6lEvI/rzga1Vta2qXgHuBC6bUGcJcD9AVT0JLExycl9bKkk6KL0E/SnAs13r25tt3TYAlwMkOR84HVjQlBXw3STrkiyf6kmSLE8ylmRsfHy81/ZLkqbRS9BP9i1KE79E40vAiUnWA9cBjwO7m7J3VdW5wCXANUnePdmTVNWtVTVaVaPz58/vrfWSpGn18qVm24FTu9YXADu6K1TVT4GrANL5er2nm4Wq2tH8fC7JGjpTQQ8fcsslST3pZUS/FliU5IwkRwJXAHd3V0hyQlMG8Eng4ar6aZJjkhzX1DkGuAjY3L/mS5KmM+2Ivqp2J7kWuA8YAVZV1ZYkVzflK4HFwO1J9gBPAMua3U8G1jTfoX048I2qurf/L0OSNJWevo++qu4B7pmwbWXX4z8DFk2y3zbg7ENsoyTpEPjJWElqOYNeklrOoJekljPoJanlDHpJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklquZ6+vVKazLx583jhhRf6drzm66wP2Yknnsjzzz/fl2NJbWDQ66C98MILVE28q+Tg9esNQ2oLp24kqeUMeklqOYNeklrOoJekljPoJanlDHpJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeV6CvokFyd5KsnWJNdPUn5ikjVJNib5QZKlve4rSZpZ0wZ9khHgFuASYAlwZZIlE6p9HlhfVW8HPg585QD2lSTNoF5G9OcDW6tqW1W9AtwJXDahzhLgfoCqehJYmOTkHveVJM2gXoL+FODZrvXtzbZuG4DLAZKcD5wOLOhxX5r9licZSzI2Pj7eW+sPwrx580hyyEvT5r4s8+bNm7HXq9mjX+em52fHMPbnoPqylztMTXa7nom3FfoS8JUk64FNwOPA7h737WysuhW4FWB0dHTGbls0jHdF2nciaW4bxnMTZu/5OYz9Oai+7CXotwOndq0vAHZ0V6iqnwJXAaTzSp5uljdNt68kaWb1MnWzFliU5IwkRwJXAHd3V0hyQlMG8Eng4Sb8p91XkjSzph3RV9XuJNcC9wEjwKqq2pLk6qZ8JbAYuD3JHuAJYNn+9p2ZlyJJmkyGbQ4LOnP0Y2NjM3LsJEM5bzdsberFsLZ7WNs1nWFt97C2azrD2O6ZbFOSdVU1OlmZn4yVpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklqOYNekvZj/KVxPnHvJ9j51zsH3ZSDZtBL0n6s3LiSH/7lD1m5YeWgm3LQDHpJmsL4S+N8e+u3KYq7tt41a0f1Br0kTWHlxpXsrb0A7K29s3ZUb9Br4NowB6r22Tea37V3FwC79u6ataN6g14D14Y5ULVP92h+n9k6qjfoNVBtmQNV+2x4bsPPR/P77Nq7i/XPrR9Qiw5eL3eYkmbMZHOgX3jHFwbcKgm+eek3B92EvnFEr4Fp0xzoMPGahyYy6DUwbZoDHSZe89BEBr0Gpk1zoMPCax6ajHP0Gpg2zYEOC695aDJz7p6xfPH4mTnuofrii4NuwYEb1r6EOdmf4yOHccmCt/I3h736i/ov7N3Lvdt3cNKevfvZs5e2zb3+nDEz1Jf7u2fsnAv6ftycd/ylcT778Gf58nu+zElHnzQUbRqEYW33sLZrOofa7hseu4E1P17zmumwIw47gssXXX5Io/q52p8zwZuDzyJe7NIw8pqHpuIc/QGaeLHr6rOv7suoXjpUXvPQVBzRH6C2fMmRpLnDoD8AfsBH0mxk0B8AP+AjaTYy6A+AF7skzUZejD0AXuySNBv1NKJPcnGSp5JsTXL9JOXHJ/lOkg1JtiS5qqvsmSSbkqxPMkOfgpIkTWXaEX2SEeAW4IPAdmBtkrur6omuatcAT1TVh5PMB55KckdVvdKUv7eqvGIpSQPQy4j+fGBrVW1rgvtO4LIJdQo4LkmAY4Hngd19bakk6aD0EvSnAM92rW9vtnW7GVgM7AA2AZ+q+vmfpxTw3STrkiw/xPZKkg5QL0GfSbZN/LKGDwHrgbcC5wA3J/nFpuxdVXUucAlwTZJ3T/okyfIkY0nGxsfHe2u9JGlavQT9duDUrvUFdEbu3a4CvlUdW4GngTMBqmpH8/M5YA2dqaDXqapbq2q0qkbnz59/YK9CkjSlXoJ+LbAoyRlJjgSuAO6eUOcvgPcDJDkZeBuwLckxSY5rth8DXARs7lfjJUnTm/avbqpqd5JrgfuAEWBVVW1JcnVTvhK4AbgtySY6Uz2fq6qdSf4OsKZzjZbDgW9U1b0z9FokSZPo6QNTVXUPcM+EbSu7Hu+gM1qfuN824OxDbKMk6RD4FQiS1HIGvSS1nEEvSS1n0EtSyxn0ktRyBr0ktZxBL0ktNydvPNJ8gGtonHjiiYNuwkEbtr4E+7Pf7M/+GVRfzrmgr5r4fWwHJ0nfjjVb9fP125/2Z7/Zn69y6kaSWs6gl6SWM+glqeUMeklqOYNeklrOoJekljPoJanlDHpJajmDXpJazqCXpJYz6CWp5Qx6SWo5g16SWs6gl6SWM+glqeUMeklqOYNeklrOoJekljPoJanlegr6JBcneSrJ1iTXT1J+fJLvJNmQZEuSq3rdV5I0s6YN+iQjwC3AJcAS4MokSyZUuwZ4oqrOBi4E/kOSI3vcV5I0g3oZ0Z8PbK2qbVX1CnAncNmEOgUclyTAscDzwO4e95UkzaBegv4U4Nmu9e3Ntm43A4uBHcAm4FNVtbfHfSVJM6iXoM8k22rC+oeA9cBbgXOAm5P8Yo/7dp4kWZ5kLMnY+Ph4D82S5p4kPS291tXc0EvQbwdO7VpfQGfk3u0q4FvVsRV4Gjizx30BqKpbq2q0qkbnz5/fa/ulOaWq+rpobugl6NcCi5KckeRI4Arg7gl1/gJ4P0CSk4G3Adt63FeSNIMOn65CVe1Oci1wHzACrKqqLUmubspXAjcAtyXZRGe65nNVtRNgsn1n5qVIkiaTYfz1bXR0tMbGxgbdjP1K4q++fWR/apjNhvMzybqqGp2szE/GSlLLGfSS1HIGvSS1nEEvSS1n0EtSyxn0ktRyBr0ktZxBL0ktZ9BLUssZ9JLUcga9JLWcQS9JLWfQS1LLGfSS1HIGvSS1nEEvSS1n0EtSyxn0ktRyBr0ktdy0Nwefi5L0td6w32typtmfGla9nnO91h3Wc9Ogn8Sw/mPNVvanhtVcOTedupGkljPoJanlDHpJajmDXpJazqCXpJYz6CWp5Qx6SWq5noI+ycVJnkqyNcn1k5R/Nsn6ZtmcZE+SeU3ZM0k2NWVj/X4BkqT9m/YDU0lGgFuADwLbgbVJ7q6qJ/bVqaqbgJua+h8GPl1Vz3cd5r1VtbOvLZck9aSXEf35wNaq2lZVrwB3Apftp/6VwOp+NE6SdOh6CfpTgGe71rc3214nyZuAi4H/3rW5gO8mWZdk+cE2VJJ0cHr5rpvJvslnqi+I+DDw6IRpm3dV1Y4kfwv4XpInq+rh1z1J501gOcBpp53WQ7MkSb3oZUS/HTi1a30BsGOKulcwYdqmqnY0P58D1tCZCnqdqrq1qkaranT+/Pk9NEuS1Itegn4tsCjJGUmOpBPmd0+slOR44D3At7u2HZPkuH2PgYuAzf1ouCSpN9NO3VTV7iTXAvcBI8CqqtqS5OqmfGVT9aPAd6vqZ127nwysab7H+XDgG1V1bz9fgCRp/zKM38c8OjpaY2P+yb0k9SrJuqoanazMT8ZKUssZ9JLUcga9JLWcQS9JLWfQH6DVq1ezdOlSRkZGWLp0KatX+20PGh6en5pML5+MVWP16tWsWLGCr33ta1xwwQU88sgjLFu2DIArr7xywK3TXOf5qSlV1dAt5513Xg2js846qx544IHXbHvggQfqrLPOGlCLpFd5fs5twFhNkan+Hf0BGBkZ4eWXX+aII474+bZdu3Zx1FFHsWfPngG2TPL8nOv8O/o+Wbx4MY888shrtj3yyCMsXrx4QC2SXuX5qakY9AdgxYoVLFu2jAcffJBdu3bx4IMPsmzZMlasWDHopkmen5qSF2MPwL4LWtdddx0/+tGPWLx4MTfeeKMXujQUPD81FefoJakFnKOXpDnMoJekljPoJanlDHpJajmDXpJabij/6ibJOPDng27HNE4Cdg66ES1if/aX/dlfs6E/T6+q+ZMVDGXQzwZJxqb6UyYdOPuzv+zP/prt/enUjSS1nEEvSS1n0B+8WwfdgJaxP/vL/uyvWd2fztFLUss5opekljPoJanlDHoNjSQPJRltHt+T5IRBt0naJ8nnD3H/jyRZ0q/2HAiDXkOpqn6jqv7voNsxKEkWJvknPdRbnWRjkk+/Ee3qVZILk7xz0O3os0MKeuAjgEE/KM1/qieT/FGSzUnuSPKBJI8m+XGS85Mck2RVkrVJHk9yWde+/yPJD5vlnc32C5sR6jebY9+RJIN9pf13iH13dJI7m6D6Y+DoruM+k+Sk5vFdSdYl2ZJkeVedv0pyY5INSR5LcvIb3gEzZyGw36BP8reBd1bV26vqP04oG/RNhS4EBhL0ST7enFMbknw9yelJ7m+23Z/ktKbebUm+muRPk2xL8lvN9rckeTjJ+uac/vUkXwKObrbd0dTr+bxscuFS4KbmGL/0hnbKVHcNn0sLnf9Uu4FfpfPmtw5YBQS4DLgL+HfAP23qnwD8T+AY4E3AUc32RTR3Yqdzor8ILGiO+WfABYN+rUPWd/8SWNVsf3tznNFm/RngpObxvObn0cBm4M3NegEfbh7/e+ALA+6HJ4E/atp4B/AB4FHgx8D5wLymPzYCjwFvb/Z9D7C+WR4HjmvKX2y2fXqK59wI/HVT59eBh5q+/j7we8B5zeN1wH3AW5r9zgM2NOfkTcDmZvsngJu7jv8nwIXN44ua+j8E/htwbNe/079ptm8Czmz64n8D/2tf297Af4ezgKe6zx3gO8DvNOv/DLireXxb81oOozPS3tps/z1gRfN4BDiuefxXE57rgM7L5vl+axDn56Df9YfJ01W1CSDJFuD+qqokm+icuAuAS5N8pql/FHAasAO4Ock5wB7gV7qO+YOq2t4cc31znNfevbkdDrbv3g18FaCqNibZOMXxfzfJR5vHp9J5Q/0/wCt0wgg6YfbBvr6qA/fLwD8ElgNr6YzIL6Azkvs88CzweFV9JMn7gNuBc4DPANdU1aNJjgVeBq4HPlNV/2A/z3cp8CdVdQ5A8wvjCVX1niRH0An5y6pqPMk/Bm6kE3T/Fbiuqr6f5KbpXlTzm9UXgA9U1c+SfI7Om/S/barsrKpzk/yLps2fTLKSTjB+uaee65/3Ad+sqp0AVfV8kl8DLm/Kv04nfPe5q6r2Ak90/Ua4FljV9OFdVbV+iueaLeelQd/lb7oe7+1a30unn/YAv1lVT3XvlOSLwF8CZ9MZGbw8xTH30N7+Pti+g87oZ0pJLqQzMv61qnopyUN03igAdlUzVGI4+ne6N7zTgd8EqKoHkrw5yfF0Rv1/0EwJfKuqth/CLN8fNz/fBiwFvtccawT4SfN8J1TV95t6XwcumeaY76Az4n20OdaRdEb3+3yr+bmOVwN1UMI059SE8u5zt3NCVj2c5N3A3we+nuSmqrr9NU8yu85L5+gPwH3Adfvm2ZP83Wb78cBPmlHBb9P5D6XXmqrvHgY+1mxbSmf6ZqLjgRea/0xn0gmdYTXdG95k6V1V9SXgk3SmAB5rXufB+lnzM8CWqjqnWX61qi5i/0G4m9dmwr7gCvC9rmMtqaplXfX2vc5hCLX7gX+U5M0ASeYBfwpc0ZR/jGl+q05yOvBcVf0X4GvAuU3RrmaUDwd3Xv4/OtNybziDvnc3AEcAG5NsbtYB/jPwO0keozNt87Mp9p/Lpuq7PwSObaZs/hXwg0n2vRc4vKlzA52569mq+43tQjpTHj9N8ktVtamqfh8YozPPfaih8BQwv5m2IMkRSc6qzl8yvZjkgqbex7r2eQY4J8lhSU6lc10BOn3+riS/3BzrTUm6pygnM5BQq6otdKaovp9kA/AHwO8CVzXn0G8Dn5rmMBcC65M8Tuc3sK8022+lcw7fwcGdl3cCn03nDxK8GOviMtsWOlMzm7vWb6O58LavjM6FwW/z+oux/6kp3wCsBn6Bzhvj/c22qS7GTnzOh2guZjfr59B5c9kAbAH+ebO9+2LsF3n1YmzoXETeQmcK6CFevRj7Pjpz1xub5dJm+zO8euFzFHioefwrTb039GKsy+SL33UjzWFJFtK5oLt0wE3RDHLqRpJazhG9NOSSfAj4/Qmbn66qj05WX5rIoJeklnPqRpJazqCXpJYz6CWp5Qx6SWq5/w+VQz80imi7iQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## evaluate model on different imputation strategies\n",
    "results = list()\n",
    "strategies = ['mean', 'median', 'most_frequent', 'constant']\n",
    "\n",
    "for s in strategies:\n",
    "    ## create pipeline\n",
    "    pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)),\n",
    "                               ('m', RandomForestClassifier())])\n",
    "    ## evaluate model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10,\n",
    "                                 n_repeats=3,\n",
    "                                 random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y,\n",
    "                             scoring='accuracy',\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1)\n",
    "    \n",
    "    ## store results using empty list created above\n",
    "    results.append(scores)\n",
    "    ## print results in the loop to get results of each strategy\n",
    "    print('>%s %.3f, (%.3f)' % (s, mean(scores), std(scores)))\n",
    "\n",
    "## plot performance for each strategy\n",
    "pyplot.boxplot(results,\n",
    "               labels=strategies,\n",
    "               showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Imputer: Making a Prediction with Best Strategy\n",
    "- after identifying the best strategy using script above, we re-run the algrithm and predict\n",
    "- Note: still need to use pipeline so imputation can be applied to each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import nan\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we create new data that will be tested once the model is built\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "## load data\n",
    "data16 = read_csv(url, header=None, na_values='?')\n",
    "data = data16.values ## convert to array\n",
    "\n",
    "## get col index for dep vars and split data into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "## create pipeline\n",
    "pipeline = Pipeline(steps=[('i', SimpleImputer(strategy='constant')),\n",
    "                           ('m', RandomForestClassifier())])\n",
    "\n",
    "## fit data\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "## predict using sample observation created above\n",
    "dataPredict = pipeline.predict([row])\n",
    "print('Predicted Class: %d' % dataPredict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Imputation (nearest neighbor imputation)\n",
    "- often involves selecting Euclidean distance and number of contributing neighbors for each prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
      "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
      "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
      "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
      "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
      "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
      "\n",
      "    20   21   22  23     24  25  26  27  \n",
      "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
      "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
      "2  NaN  NaN  1.0   2      0   0   0   1  \n",
      "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
      "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "data17 = read_csv(url, header=None, na_values='?')\n",
    "print(data17.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 1 (0.3%)\n",
      "> 1, Missing: 0 (0.0%)\n",
      "> 2, Missing: 0 (0.0%)\n",
      "> 3, Missing: 60 (20.0%)\n",
      "> 4, Missing: 24 (8.0%)\n",
      "> 5, Missing: 58 (19.3%)\n",
      "> 6, Missing: 56 (18.7%)\n",
      "> 7, Missing: 69 (23.0%)\n",
      "> 8, Missing: 47 (15.7%)\n",
      "> 9, Missing: 32 (10.7%)\n",
      "> 10, Missing: 55 (18.3%)\n",
      "> 11, Missing: 44 (14.7%)\n",
      "> 12, Missing: 56 (18.7%)\n",
      "> 13, Missing: 104 (34.7%)\n",
      "> 14, Missing: 106 (35.3%)\n",
      "> 15, Missing: 247 (82.3%)\n",
      "> 16, Missing: 102 (34.0%)\n",
      "> 17, Missing: 118 (39.3%)\n",
      "> 18, Missing: 29 (9.7%)\n",
      "> 19, Missing: 33 (11.0%)\n",
      "> 20, Missing: 165 (55.0%)\n",
      "> 21, Missing: 198 (66.0%)\n",
      "> 22, Missing: 1 (0.3%)\n",
      "> 23, Missing: 0 (0.0%)\n",
      "> 24, Missing: 0 (0.0%)\n",
      "> 25, Missing: 0 (0.0%)\n",
      "> 26, Missing: 0 (0.0%)\n",
      "> 27, Missing: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "## summarize missing data\n",
    "for i in range(data17.shape[1]):\n",
    "    n_miss = data17[[i]].isnull().sum()\n",
    "    perc_miss = n_miss / data17.shape[0] * 100\n",
    "    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc_miss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Imputation with KNNImputer\n",
    "- default distance measurement is Euclidian distance that is NaN aware\n",
    "- number of neighbors default is 5\n",
    "- distance measured can be weighted proportional to distance between rows\n",
    "    - default is uniform weight, but can be controlled via 'weights' argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import isnan\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "data18 = read_csv(url, header=None, na_values='?')\n",
    "data = data18.values\n",
    "\n",
    "## get column indexes for dep var columns \n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "## split into dep and indep vars\n",
    "X, y = data[:, Xdata], data[:, 23]\n",
    "\n",
    "## summarize missing data in dep vars\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "## define imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "## fit on data and transform values to the data\n",
    "imputer.fit(X)\n",
    "XImp = imputer.transform(X)\n",
    "\n",
    "## summarize missing data after transform\n",
    "print('Missing: %d' % sum(isnan(XImp).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data19 = read_csv(url, header=None, na_values='?')\n",
    "data = data19.values\n",
    "\n",
    "## get dep var col indices and split into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.867, (0.055)\n"
     ]
    }
   ],
   "source": [
    "## define pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = KNNImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "## define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "## evaluate the model\n",
    "dataEval = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=1)\n",
    "print('Mean Accuracy: %.3f, (%.3f)' % (mean(dataEval), std(dataEval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Imputation with KNNImputer different Neighbors\n",
    "- the number of neighbors can impact results so we can test multipe neighbor values\n",
    "- remember default (used above) is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data20 = read_csv(url, header=None, na_values='?')\n",
    "data = data20.values\n",
    "## get column indices and split data into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 0.860 (0.052)\n",
      ">3 0.863 (0.060)\n",
      ">5 0.868 (0.055)\n",
      ">7 0.864 (0.059)\n",
      ">9 0.860 (0.057)\n",
      ">15 0.860 (0.053)\n",
      ">18 0.856 (0.057)\n",
      ">21 0.854 (0.057)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "strategies = [str(i) for i in [1, 3, 5, 7, 9, 15, 18, 21]]\n",
    "\n",
    "for s in strategies:\n",
    "    ## create pipeline\n",
    "    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))),\n",
    "                               ('m', RandomForestClassifier())])\n",
    "    ## evaluate model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y,\n",
    "                             scoring='accuracy',\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1)\n",
    "    results.append(scores)\n",
    "    print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n",
    "\n",
    "## plot scores for comparison\n",
    "pyplot.boxplot(results, labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNNImputer: Making a Prediction with Best Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define sample data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, \n",
    "       nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, \n",
    "       00000, 00000, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data21 = read_csv(url, header=None, na_values='?')\n",
    "data = data21.values\n",
    "\n",
    "## identify dep var column indices and split into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "## create pipeline\n",
    "## note: from test above, n=5 had best results so we specify that here\n",
    "pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=5)),\n",
    "                           ('m', RandomForestClassifier())])\n",
    "\n",
    "## fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "## predict on new data (see above)\n",
    "dataPred = pipeline.predict([row])\n",
    "\n",
    "## summarize prediciton\n",
    "print('Predicted Class: %d' % dataPred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "797px",
    "left": "62px",
    "top": "53px",
    "width": "416.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for ML:\n",
    " \n",
    "- From: Jason Brownlee\n",
    "- MML Book: 'Data Preparation for Machine Learning' v1.1, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Overview:\n",
    "## Steps:\n",
    "1. Define problem\n",
    "2. Prepare Data\n",
    "3. Eval Models\n",
    "4. Finalize Model\n",
    "\n",
    "## Reminder:\n",
    "- ML Algorithms require data to be numbers\n",
    "- Some ML Algorithms impost requirements on the data \n",
    "    - specific distribution needed, \n",
    "    - removal of correlative, \n",
    "    - non-preditive vars,\n",
    "    - redundant vars\n",
    "- Statistical noise and errors in data may need to be corrected\n",
    "- Non-linear relationships may need to be identified\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepartion Tasks:\n",
    "- cleaning\n",
    "    - redundant samples and features\n",
    "    - extreme values (outliers)\n",
    "    - mark and impute missing data\n",
    "- feature selection\n",
    "    - unsupervised\n",
    "    - supervised: trees, RFE, statistics\n",
    "    - \n",
    "- transforms\n",
    "    - Numerical Type:\n",
    "        - Change Distribution:\n",
    "            - descretization transform: encode numeric to ordinal\n",
    "            - power: distribution of a variable to more Gausian\n",
    "            - quantile: impose probability distribution such as uniform or Gaussian\n",
    "        - Change Scale:\n",
    "            - normalize: scale variable to range 0-1\n",
    "            - standardization: scale variable to standard Gaussian\n",
    "            - robust\n",
    "        - Engineer:\n",
    "            - polynomial (see feature engineering below)\n",
    "    - Categorical Type:\n",
    "        - Nominal Type:\n",
    "            - one-hot encode: encode categorical to binary\n",
    "            - dummy encode\n",
    "        - Ordinal Type:\n",
    "            - ordinal encode: encode categorical to integer\n",
    "        \n",
    "     \n",
    "- feature engineering\n",
    "    - add boolean flag, group or global summary statistic, create new vars\n",
    "    - polynomial transforms: create copies of numerical input vars that are raised to a power\n",
    "- dimensionality reduction\n",
    "    - Matrix Factorization:\n",
    "        - PCA\n",
    "        - SVD (singular value decomposition)\n",
    "    - Maniforld Learning:\n",
    "        - SOM (self-organizing maps)\n",
    "        - tSNE (t-distributed Stochastic Neighbor Embedding)\n",
    "    - Model Based:\n",
    "        - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation without Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "\n",
    "- leakage occurs when knowledge of hold-out test set leaks into training set because you didn't split them first\n",
    "- data prep must be fit on training dataset only\n",
    "\n",
    "- Correct process:\n",
    "    - split data\n",
    "    - fit data pre on training data\n",
    "    - apply data prep to train and test data\n",
    "    - eval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Example with Naive Data Preparation\n",
    "- INCORRECT way that doesn't split first\n",
    "    - this is to compare results to the correct process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "## define dataset\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=20,\n",
    "                           n_informative=15,\n",
    "                           n_redundant=5,\n",
    "                           random_state=7)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we are scaling before the split creating data leakageabs\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "## split scaled data inot train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.848\n"
     ]
    }
   ],
   "source": [
    "## fit logistic model on train set\n",
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(X_train, y_train)\n",
    "\n",
    "## evaluate model using test set\n",
    "yhatLR = modelLR.predict(X_test)\n",
    "accuracyLR = accuracy_score(y_test, yhatLR)\n",
    "\n",
    "print('Accuracy: %.3f' % (accuracyLR*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Example with Correct Data Preparation\n",
    "- split first, then apply data prep to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.33,\n",
    "                                                   random_state=1)\n",
    "\n",
    "\n",
    "## define scaler and fit on training data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "## scale training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#print(X_train[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.455\n"
     ]
    }
   ],
   "source": [
    "modelLR2 = LogisticRegression()\n",
    "modelLR2.fit(X_train, y_train)\n",
    "\n",
    "## evaluate model against test set\n",
    "yhatLR2 = modelLR2.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, yhatLR2)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation Data Preparation\n",
    "\n",
    "- in this example, used on synthetic binary classification\n",
    "- k-fold algorithm: split data into k non-overlapping groups of rows\n",
    "    - model trained on all but one group to form training dataset\n",
    "    - repeated so each fold is given opp to be the hold-out set\n",
    "    - average performace across all runs is used \n",
    "- generally more reliable than train-test split but also computationally expensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Example with Naive Preparation:\n",
    "- INCORRECT method would be to apply data prep algorithm prior \n",
    "- involves apply data transforms first and then using the cross-validation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.300 (3.607)\n"
     ]
    }
   ],
   "source": [
    "## create sample data\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=20,\n",
    "                           n_informative=15,\n",
    "                           n_redundant=5,\n",
    "                           random_state=7)\n",
    "\n",
    "\n",
    "\n",
    "## standarize the data, fit on X, then transform on X\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X) #first fit, then transform (combines 2 steps)\n",
    "\n",
    "## define model\n",
    "modelLR = LogisticRegression()\n",
    "\n",
    "## define eval procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,\n",
    "                             random_state=1)\n",
    "\n",
    "scores = cross_val_score(modelLR, X, y,\n",
    "                         scoring='accuracy',\n",
    "                         cv=cv,\n",
    "                         n_jobs=1)\n",
    "\n",
    "## report accuracy\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Example with Correct Data Preparation:\n",
    "- Requires that data prep method is prepared on training and test sets within the cv procedure\n",
    "    - Requires pipeline to do this for each iteration of the procedure\n",
    "    - This can be achieved using the Pipeline class. \n",
    "        - This class takes a list of steps that define the pipeline. \n",
    "        - Each step in the list is a tuple with two elements. \n",
    "            - the first element is the name of the step (a string) and \n",
    "            - the second is the configured object of the step, such as a transform or a model. \n",
    "        - The model is only supported as the final step, \n",
    "            - although we can have as many transforms as we like in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.433 (3.471)\n"
     ]
    }
   ],
   "source": [
    "## create sample data\n",
    "X, y = make_classification(n_samples=1000,\n",
    "                           n_features=20,\n",
    "                           n_informative=15,\n",
    "                           n_redundant=5,\n",
    "                           random_state=7)\n",
    "\n",
    "## define pipeline\n",
    "steps = list()  # see description of tuples in list, above\n",
    "steps.append(('scalar', MinMaxScaler()))\n",
    "steps.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "## define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,\n",
    "                             random_state=1)\n",
    "\n",
    "## evaluate model using cross validation\n",
    "scores = cross_val_score(pipeline, X, y, \n",
    "                         scoring='accuracy',\n",
    "                         cv=cv,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "## print accuracy results\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "- Messy Data\n",
    "    - example data sets to play with: \n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.names\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv\n",
    "        - https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.names\n",
    "- ID and Delete columns containing single variable\n",
    "- Consider columns with few variables\n",
    "- Remove columns with low variance\n",
    "- Identify and Delete rows containing duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Single Variable Columns\n",
    "- ie: low or zero variance, which have no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "data = loadtxt(url, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 238\n",
      "1 297\n",
      "2 927\n",
      "3 933\n",
      "4 179\n",
      "5 375\n",
      "6 820\n",
      "7 618\n",
      "8 561\n",
      "9 57\n",
      "10 577\n",
      "11 59\n",
      "12 73\n",
      "13 107\n",
      "14 53\n",
      "15 91\n",
      "16 893\n",
      "17 810\n",
      "18 170\n",
      "19 53\n",
      "20 68\n",
      "21 9\n",
      "22 1\n",
      "23 92\n",
      "24 9\n",
      "25 8\n",
      "26 9\n",
      "27 308\n",
      "28 447\n",
      "29 392\n",
      "30 107\n",
      "31 42\n",
      "32 4\n",
      "33 45\n",
      "34 141\n",
      "35 110\n",
      "36 3\n",
      "37 758\n",
      "38 9\n",
      "39 9\n",
      "40 388\n",
      "41 220\n",
      "42 644\n",
      "43 649\n",
      "44 499\n",
      "45 2\n",
      "46 937\n",
      "47 169\n",
      "48 286\n",
      "49 2\n"
     ]
    }
   ],
   "source": [
    "## summarize number of unique values in each column OPTION 1\n",
    "for i in range(data.shape[1]):\n",
    "    print(i, len(unique(data[:, i])))\n",
    "    \n",
    "## here we can look for columsn that have 1 or low number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     238\n",
      "1     297\n",
      "2     927\n",
      "3     933\n",
      "4     179\n",
      "5     375\n",
      "6     820\n",
      "7     618\n",
      "8     561\n",
      "9      57\n",
      "10    577\n",
      "11     59\n",
      "12     73\n",
      "13    107\n",
      "14     53\n",
      "15     91\n",
      "16    893\n",
      "17    810\n",
      "18    170\n",
      "19     53\n",
      "20     68\n",
      "21      9\n",
      "22      1\n",
      "23     92\n",
      "24      9\n",
      "25      8\n",
      "26      9\n",
      "27    308\n",
      "28    447\n",
      "29    392\n",
      "30    107\n",
      "31     42\n",
      "32      4\n",
      "33     45\n",
      "34    141\n",
      "35    110\n",
      "36      3\n",
      "37    758\n",
      "38      9\n",
      "39      9\n",
      "40    388\n",
      "41    220\n",
      "42    644\n",
      "43    649\n",
      "44    499\n",
      "45      2\n",
      "46    937\n",
      "47    169\n",
      "48    286\n",
      "49      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## summarize number of unique values in each column OPTION 2\n",
    "## this method uses pandas nunique function (so read_csv to read in as df)\n",
    "data1 = read_csv(url, header=None)\n",
    "print(data1.nunique())\n",
    "## here we can look for columsn that have 1 or low number of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Single Var Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[22]\n",
      "(937, 49)\n"
     ]
    }
   ],
   "source": [
    "data1 = read_csv(url, header=None)\n",
    "print(data1.shape)\n",
    "\n",
    "## get number of unique values for each col\n",
    "unqCnt = data1.nunique()\n",
    "\n",
    "## identify columns (by index number) to delete (those cols with only 1 value)\n",
    "## enumerate loops through list and adds a index/counter\n",
    "## i is the index/counter and v is the nunique value \n",
    "toDel = [i for i, v in enumerate(unqCnt) if v==1]\n",
    "print(toDel)\n",
    "\n",
    "## delete columns with only 1 valua\n",
    "data1.drop(toDel, axis=1, inplace=True)\n",
    "print(data1.shape)\n",
    "## in this example, col 22 had only one value: therefor column count goes from 50 to 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Columns with Few Values\n",
    "- it may make sense for categorical variables to have few values but less so for numerical columns\n",
    "    - these may have little to no variance and therefore, less predictive of dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 238, 25.4%\n",
      "1, 297, 31.7%\n",
      "2, 927, 98.9%\n",
      "3, 933, 99.6%\n",
      "4, 179, 19.1%\n",
      "5, 375, 40.0%\n",
      "6, 820, 87.5%\n",
      "7, 618, 66.0%\n",
      "8, 561, 59.9%\n",
      "9, 57, 6.1%\n",
      "10, 577, 61.6%\n",
      "11, 59, 6.3%\n",
      "12, 73, 7.8%\n",
      "13, 107, 11.4%\n",
      "14, 53, 5.7%\n",
      "15, 91, 9.7%\n",
      "16, 893, 95.3%\n",
      "17, 810, 86.4%\n",
      "18, 170, 18.1%\n",
      "19, 53, 5.7%\n",
      "20, 68, 7.3%\n",
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "23, 92, 9.8%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "27, 308, 32.9%\n",
      "28, 447, 47.7%\n",
      "29, 392, 41.8%\n",
      "30, 107, 11.4%\n",
      "31, 42, 4.5%\n",
      "32, 4, 0.4%\n",
      "33, 45, 4.8%\n",
      "34, 141, 15.0%\n",
      "35, 110, 11.7%\n",
      "36, 3, 0.3%\n",
      "37, 758, 80.9%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "40, 388, 41.4%\n",
      "41, 220, 23.5%\n",
      "42, 644, 68.7%\n",
      "43, 649, 69.3%\n",
      "44, 499, 53.3%\n",
      "45, 2, 0.2%\n",
      "46, 937, 100.0%\n",
      "47, 169, 18.0%\n",
      "48, 286, 30.5%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "## here we summarize the percentage of unique values for each column\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "\n",
    "## define data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "## load data\n",
    "data2 = loadtxt(url, delimiter=',')\n",
    "\n",
    "## summarize unique values for each column\n",
    "for i in range(data2.shape[1]):\n",
    "    num = len(unique(data2[:, i]))\n",
    "    perc = float(num) / data2.shape[0] * 100\n",
    "    print('%d, %d, %.1f%%' % (i, num, perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "32, 4, 0.4%\n",
      "36, 3, 0.3%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "45, 2, 0.2%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "## Identify only those variables that are < 1%\n",
    "## once identified, these need to be further analyzed before deciding to delete or not\n",
    "for i in range(data2.shape[1]):\n",
    "    num = len(unique(data2[:, i]))\n",
    "    perc = float(num) / data.shape[0] * 100\n",
    "    if perc < 1:\n",
    "        print('%d, %d, %.1f%%' % (i, num, perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]\n",
      "(937, 39)\n"
     ]
    }
   ],
   "source": [
    "## In this example, we chose to delete all 11 columns identified above \n",
    "\n",
    "data3 = read_csv(url, header=None)\n",
    "## get unique counts for each column \n",
    "counts = data3.nunique()\n",
    "\n",
    "## identify columns to delete \n",
    "toDel2 = [i for i, v in enumerate(counts) if (float(v)/data3.shape[0] * 100) < 1]\n",
    "## print column numbers to delete \n",
    "print(toDel2)\n",
    "\n",
    "data3.drop(toDel2, axis=1, inplace=True)\n",
    "print(data3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Columns with Low Variance\n",
    "- another (probably better approach) is to consider variance of the column\n",
    "- variance = av squared difference of values in sample, from the mean\n",
    "    - column with single value has variance of 0\n",
    "    \n",
    "- VarianceThreshold class from scikit-learn supports this as a type of feature selection\n",
    "    - default is 0.0 but we can specify a threshold, then fit and apply using fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 49) (937,)\n",
      "(937, 48)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "## load data\n",
    "data4 = read_csv(url, header=None)\n",
    "\n",
    "## split data into input and output \n",
    "data4 = data4.values\n",
    "X = data4[:, :-1]\n",
    "y = data4[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## define transform\n",
    "transf = VarianceThreshold()\n",
    "\n",
    "## transform input data \n",
    "## because we don't specify threshold above, uses 0.0 default %& removes cols with 0.0 variance\n",
    "X_sel = transf.fit_transform(X)\n",
    "print(X_sel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Threshold=0.00, Features=48\n",
      ">Threshold=0.05, Features=37\n",
      ">Threshold=0.10, Features=36\n",
      ">Threshold=0.15, Features=35\n",
      ">Threshold=0.20, Features=35\n",
      ">Threshold=0.25, Features=35\n",
      ">Threshold=0.30, Features=35\n",
      ">Threshold=0.35, Features=35\n",
      ">Threshold=0.40, Features=35\n",
      ">Threshold=0.45, Features=33\n",
      ">Threshold=0.50, Features=31\n"
     ]
    }
   ],
   "source": [
    "## repeat above but specify thresholds that are placed in a list \n",
    "from numpy import arange\n",
    "from matplotlib import pyplot\n",
    "## define thresholds to test \n",
    "thresholds = arange(0.0, 0.55, 0.05)\n",
    "\n",
    "## apply transform with each threshold \n",
    "results = list()\n",
    "for t in thresholds:\n",
    "    ## define the transform \n",
    "    transf = VarianceThreshold(threshold=t)\n",
    "    ## transform input data \n",
    "    X_sel = transf.fit_transform(X)\n",
    "    ## determine how many input features there are for each threshold \n",
    "    n_feats = X_sel.shape[1]\n",
    "    print('>Threshold=%.2f, Features=%d' % (t, n_feats))\n",
    "    results.append(n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot relationship between threshold and results \n",
    "##   notice steep drops: 0.0-0.05 and 0.4-0.5 \n",
    "pyplot.plot(thresholds, results)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Duplicate Data\n",
    "- using train/test/split or k-fold cv could result in same row being in the train and eval of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "irisURL = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "       0    1    2    3               4\n",
      "34   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "37   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "irisData = read_csv(irisURL, header=None)\n",
    "\n",
    "## identify duplicates \n",
    "dups = irisData.duplicated()\n",
    "print(dups.any())  ## True/False whether there are any dups\n",
    "print(irisData[dups]) ## print rows containing the duplicates\n",
    "## note 3 duplicated rows are identified in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Rows with Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "print(irisData.shape)\n",
    "irisData.drop_duplicates(inplace=True)\n",
    "print(irisData.shape)\n",
    "## note: 3 dup rows were identified and it dropped 3 (150-147)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Identification and Removal\n",
    "- very simply, outliers are data that fall outside the expected range of a dataset\n",
    "    - unlikely, rare, distinct (measurement or input error, data corruption, true outlier)\n",
    "    - can have many causes\n",
    "    \n",
    "- In this tutorial:\n",
    "    - use of simple univariate statistics (stdev, IQR)\n",
    "    - use of automatic outlier detection models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gaussian Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 50.049, stdv = 4.994\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn  ## generates gaussian dist\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "seed(1)\n",
    "\n",
    "## Define tst data: \n",
    "    ## 10,000 random numbers \n",
    "    ## drawn from Gaussian distribution\n",
    "    ## mean 50, std 5\n",
    "data5 = 5 * randn(10000) + 50\n",
    "## confirm distribution specs\n",
    "print('mean = %.3f, stdv = %.3f' % (mean(data5), std(data5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation Method\n",
    "- stdev can be used if we know distribution is at least Gaussian-like\n",
    "    - within 1 stdev, we expect to see ~68% of the data\n",
    "    - within 2 stdev, we expect to see ~95% of the data\n",
    "    - within 3 stdev, we expect to see ~99.7% of the data\n",
    "- 3 stdev from mean is common cut-off for identifying outliers in Gaussian-like dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified outliers: 29\n",
      "Non-outliers: 9971\n"
     ]
    }
   ],
   "source": [
    "## calculate summary stats\n",
    "data_mean, data_std = mean(data5), std(data5)\n",
    "\n",
    "## define outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "\n",
    "## identify outliers in the dataset\n",
    "outliers = [x for x in data5 if x < lower or x > upper]\n",
    "outliersIdx = [i for i, x in enumerate(data5) if x < lower or x > upper]\n",
    "#print(outliers); print(outliersIdx)\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "## remove outliers\n",
    "outliersRem = [x for x in data5 if x > lower and x < upper]\n",
    "print('Non-outliers: %d' % len(outliersRem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interquartile Range Method\n",
    "- data is often not Gaussian: IQR is good method for summarizing non-gaussian distributions\n",
    "    - recall the 50th percentile is middle value (or average of 2 middle values)\n",
    "    - IQR is the middle 50 percent of the data\n",
    "\n",
    "- Outlier factors:\n",
    "    - common outlier factor is 1.5 above 75th or below 25th\n",
    "    - factors of 3*IQR above 75th or below 25th are extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn  ## generates gaussian dist\n",
    "from numpy import percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles: 25th = 46.685, 75 = 53.359, IQR = 6.674\n",
      "Identified outliers: 0\n",
      "Non-outliers: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed(1)\n",
    "\n",
    "## define data\n",
    "data6 = 5 * randn(10000) + 50\n",
    "\n",
    "## define outlier factor (normal, extreme)\n",
    "z = 3\n",
    "\n",
    "## calculate IQR\n",
    "q25, q75 = percentile(data5, 25), percentile(data5, 75)\n",
    "iqr = q75-q25\n",
    "print('Percentiles: 25th = %.3f, 75 = %.3f, IQR = %.3f' % (q25, q75, iqr))\n",
    "\n",
    "## calculate cutoffs\n",
    "cut_off = iqr*z\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "outliers = [x for x in data6 if x < lower or x > upper]\n",
    "#print(outliers)\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "\n",
    "## remove outliers\n",
    "outliersRem = [x for x in data6 if x > lower and x < upper]\n",
    "print('Non-outliers: %d' % len(outliersRem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Outlier Detection\n",
    "\n",
    "- One-class classification\n",
    "    - 'aims at capturing characteristics of training instances, in order to be able to distinguish between them and potential outliers to appear.\n",
    "    - locate examples that are far from other examples in multidimensional space\n",
    "        - can become less reliable as number of features increases\n",
    "    - Local outlier factor:\n",
    "        - uses idea of nearest neighbor\n",
    "        - each observation assigned score related to how isolated or how likely it is to be outlier based on size of its local neighborhood\n",
    "            - larges score are more likely to be outliers\n",
    "            \n",
    "- **Note: this example uses LocalOutlierFactor but scikitlearn also offers IsolationForest algorithm which can be used in same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "(339, 13) (167, 13) (339,) (167,)\n"
     ]
    }
   ],
   "source": [
    "## Without Outliers Removed (to be later compared to model with outliers removed)\n",
    "## define data location\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "\n",
    "data7 = read_csv(url, header=None)\n",
    "\n",
    "## create array\n",
    "data7 = data7.values\n",
    "\n",
    "## split into input and output\n",
    "X, y = data7[:, :-1], data7[:, -1]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## split into train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 3.417\n"
     ]
    }
   ],
   "source": [
    "## generate linear regression model\n",
    "\n",
    "## fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "## evaluate model\n",
    "modelPred = model.predict(X_test)\n",
    "\n",
    "## evaluate error\n",
    "mae = mean_absolute_error(y_test, modelPred)\n",
    "print('Mean Absolute Error %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "(339, 13) (339,)\n"
     ]
    }
   ],
   "source": [
    "## With Outliers Removed\n",
    "\n",
    "## read data in and convert to array\n",
    "data8 = read_csv(url, header=None)\n",
    "data8 = data8.values\n",
    "\n",
    "## split into input and output\n",
    "X, y = data7[:, :-1], data7[:, -1]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "## split into train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305, 13) (305,)\n",
      "Mean Absolute Error 3.356\n"
     ]
    }
   ],
   "source": [
    "## identify outliers in training data set\n",
    "## define method  (normal data marked with 1, outlier marked with -1)\n",
    "lof = LocalOutlierFactor()  ## uses default (see documentation to tune)\n",
    "X_trainLOF = lof.fit_predict(X_train) ## creates array of 1 or -1\n",
    "#print(X_trainLOF)\n",
    "\n",
    "## define variable to select rows that aren't outliers\n",
    "tag = X_trainLOF != -1  ## creates array of true/false\n",
    "#print(tag)\n",
    "\n",
    "## create new X and y train: observations where tag = True\n",
    "X_trainRem, y_trainRem = X_train[tag, :], y_train[tag]\n",
    "print(X_trainRem.shape, y_trainRem.shape)  ## new shape confirms outlier removal\n",
    "\n",
    "## define and fit the model\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_trainRem, y_trainRem)\n",
    "\n",
    "## predict on test set\n",
    "model2Pred = model2.predict(X_test)\n",
    "## evaluate error\n",
    "mae2 = mean_absolute_error(y_test, model2Pred)\n",
    "print('Mean Absolute Error %.3f' % mae2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "- note that some missing values may be indicated in odd ways or ways that don't make sense\n",
    "    - ex: a zero for BMI would indicate missing since no one can have zero body mass\n",
    "    - ex: a negative number in numeric field that can only be positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import arange\n",
    "## load data  (note: see section 5 for data set 'names' link)\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0           1           2           3           4           5  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "                6           7           8  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "     0    1   2   3    4     5      6   7  8\n",
      "0    6  148  72  35    0  33.6  0.627  50  1\n",
      "1    1   85  66  29    0  26.6  0.351  31  0\n",
      "2    8  183  64   0    0  23.3  0.672  32  1\n",
      "3    1   89  66  23   94  28.1  0.167  21  0\n",
      "4    0  137  40  35  168  43.1  2.288  33  1\n",
      "5    5  116  74   0    0  25.6  0.201  30  0\n",
      "6    3   78  50  32   88  31.0  0.248  26  1\n",
      "7   10  115   0   0    0  35.3  0.134  29  0\n",
      "8    2  197  70  45  543  30.5  0.158  53  1\n",
      "9    8  125  96   0    0   0.0  0.232  54  1\n",
      "10   4  110  92   0    0  37.6  0.191  30  0\n",
      "11  10  168  74   0    0  38.0  0.537  34  1\n",
      "12  10  139  80   0    0  27.1  1.441  57  0\n",
      "13   1  189  60  23  846  30.1  0.398  59  1\n",
      "14   5  166  72  19  175  25.8  0.587  51  1\n",
      "15   7  100   0   0    0  30.0  0.484  32  1\n",
      "16   0  118  84  47  230  45.8  0.551  31  1\n",
      "17   7  107  74   0    0  29.6  0.254  31  1\n",
      "18   1  103  30  38   83  43.3  0.183  33  0\n",
      "19   1  115  70  30   96  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "data9 = read_csv(url, header=None)\n",
    "print(data9.describe())\n",
    "print(data9.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    111\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8    500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## get count of missing numbers for each variable\n",
    "## zeros for some columns expected: need to review data documentation to figure out which ones\n",
    "missingCnt = (data9[arange(0,data9.shape[1])] == 0).sum()\n",
    "print(missingCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "\n",
    "data10 = read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n",
      "     0      1     2     3      4     5      6   7  8\n",
      "0    6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
      "1    1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
      "2    8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
      "3    1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
      "4    0  137.0  40.0  35.0  168.0  43.1  2.288  33  1\n",
      "5    5  116.0  74.0   NaN    NaN  25.6  0.201  30  0\n",
      "6    3   78.0  50.0  32.0   88.0  31.0  0.248  26  1\n",
      "7   10  115.0   NaN   NaN    NaN  35.3  0.134  29  0\n",
      "8    2  197.0  70.0  45.0  543.0  30.5  0.158  53  1\n",
      "9    8  125.0  96.0   NaN    NaN   NaN  0.232  54  1\n",
      "10   4  110.0  92.0   NaN    NaN  37.6  0.191  30  0\n",
      "11  10  168.0  74.0   NaN    NaN  38.0  0.537  34  1\n",
      "12  10  139.0  80.0   NaN    NaN  27.1  1.441  57  0\n",
      "13   1  189.0  60.0  23.0  846.0  30.1  0.398  59  1\n",
      "14   5  166.0  72.0  19.0  175.0  25.8  0.587  51  1\n",
      "15   7  100.0   NaN   NaN    NaN  30.0  0.484  32  1\n",
      "16   0  118.0  84.0  47.0  230.0  45.8  0.551  31  1\n",
      "17   7  107.0  74.0   NaN    NaN  29.6  0.254  31  1\n",
      "18   1  103.0  30.0  38.0   83.0  43.3  0.183  33  0\n",
      "19   1  115.0  70.0  30.0   96.0  34.6  0.529  32  1\n"
     ]
    }
   ],
   "source": [
    "## replace zeros with nan for specified columns\n",
    "##   review of columns indicates zeros aren't sensible for columns 1, 2, 3, 4, 5 \n",
    "nanCols = [1, 2, 3, 4, 5]\n",
    "data10[nanCols] = data10[nanCols].replace(0, nan)\n",
    "\n",
    "## alternative syntax\n",
    "#data10[[1, 2, 3, 4, 5]] = data10[[1, 2, 3, 4,5]].replace(0, nan)\n",
    "\n",
    "print(data10.isnull().sum())\n",
    "print(data10.head(20))  ## check to make sure NaN added correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values impact to Algorithms\n",
    "- some machine learning algorithms cannot handle missing or nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from pandas import read_csv\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 425, in fit\n",
      "    dtype=[np.float64, np.float32])\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 803, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 646, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 100, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 425, in fit\n",
      "    dtype=[np.float64, np.float32])\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 803, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 646, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 100, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py\", line 425, in fit\n",
      "    dtype=[np.float64, np.float32])\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 803, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 646, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\delos001\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 100, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "#### NOTE: THIS WILL FAIL BECAUSE OF PRESENCE OF NAN\n",
    "\n",
    "## load data\n",
    "data11 = read_csv(url, header=None)\n",
    "\n",
    "## replace zeros with nan\n",
    "nanCol = [1, 2, 3, 4, 5]\n",
    "data11[nanCol] = data11[nanCol].replace(0, nan)\n",
    "\n",
    "## convert to array\n",
    "data11 = data11.values\n",
    "## split data into input and output\n",
    "X, y = data11[:, :-1], data11[:, -1]\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "## define model and model evaluation (cv) procedure\n",
    "model = LinearDiscriminantAnalysis()\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "## evaluate model\n",
    "modelEval = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "print('Accuracy: %.3f' % modelEval.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Missing Values\n",
    "- simplest strategy to handle missing values is to remove those associated observations\n",
    "    - BUT BE CAREFUL because this could have negative effects\n",
    "    - sometimes that a value is missing can be predictive\n",
    "    - there are other strategies to impute missing values so as to not lose information via removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "Accuracy: 0.781\n"
     ]
    }
   ],
   "source": [
    "## In this example, we remove rows that have nan so the LDA can be performed\n",
    "data12 = read_csv(url, header=None)\n",
    "print(data12.shape)\n",
    "\n",
    "nanCols = [1, 2, 3, 4, 5]\n",
    "data12[nanCols] = data12[nanCols].replace(0, nan)\n",
    "## drop rows with missing values (nan's)\n",
    "data12.dropna(inplace=True)\n",
    "\n",
    "## we can now run LDA on the reduce dataset\n",
    "data12 = data12.values  ## create array\n",
    "X, y = data12[:, :-1], data12[:, -1]\n",
    "\n",
    "## define model and evaluation procedure\n",
    "model = LinearDiscriminantAnalysis()\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "## evaluate model\n",
    "modelEval = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "print('Accuracy: %.3f' % modelEval.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Imputation\n",
    "- replacing missing values with a statistical replacement:\n",
    "    - is relatively simple (common types: mean, median, mode, constant)\n",
    "    - allows you to keep the observation (rather than delete potentially useful rows)\n",
    "    - often results in good performance\n",
    "    \n",
    "- In this tutorial:\n",
    "    - statistical imputation\n",
    "    - SimpleImputer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer Class: Simple Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import isnan\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28)\n",
      "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
      "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
      "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
      "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
      "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
      "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
      "\n",
      "    20   21   22  23     24  25  26  27  \n",
      "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
      "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
      "2  NaN  NaN  1.0   2      0   0   0   1  \n",
      "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
      "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "data13 = read_csv(url, header=None, na_values='?')\n",
    "print(data13.shape)\n",
    "print(data13.head(5))  ## notice the question marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 1 (0.33%)\n",
      "> 1, Missing: 0 (0.00%)\n",
      "> 2, Missing: 0 (0.00%)\n",
      "> 3, Missing: 60 (20.00%)\n",
      "> 4, Missing: 24 (8.00%)\n",
      "> 5, Missing: 58 (19.33%)\n",
      "> 6, Missing: 56 (18.67%)\n",
      "> 7, Missing: 69 (23.00%)\n",
      "> 8, Missing: 47 (15.67%)\n",
      "> 9, Missing: 32 (10.67%)\n",
      "> 10, Missing: 55 (18.33%)\n",
      "> 11, Missing: 44 (14.67%)\n",
      "> 12, Missing: 56 (18.67%)\n",
      "> 13, Missing: 104 (34.67%)\n",
      "> 14, Missing: 106 (35.33%)\n",
      "> 15, Missing: 247 (82.33%)\n",
      "> 16, Missing: 102 (34.00%)\n",
      "> 17, Missing: 118 (39.33%)\n",
      "> 18, Missing: 29 (9.67%)\n",
      "> 19, Missing: 33 (11.00%)\n",
      "> 20, Missing: 165 (55.00%)\n",
      "> 21, Missing: 198 (66.00%)\n",
      "> 22, Missing: 1 (0.33%)\n",
      "> 23, Missing: 0 (0.00%)\n",
      "> 24, Missing: 0 (0.00%)\n",
      "> 25, Missing: 0 (0.00%)\n",
      "> 26, Missing: 0 (0.00%)\n",
      "> 27, Missing: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "## summarize missing data\n",
    "for i in range(data13.shape[1]):\n",
    "    n_miss = data13[[i]].isnull().sum()\n",
    "    per_miss = n_miss / data13.shape[0] * 100\n",
    "    print('> %d, Missing: %d (%.2f%%)' % (i, n_miss, per_miss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n"
     ]
    }
   ],
   "source": [
    "## split into dep and independ var\n",
    "\n",
    "data = data13.values ## convert to array\n",
    "\n",
    "## get index for dependent variable columns (cols 1-22 )\n",
    "dataX = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, dataX], data[:, 23]\n",
    "\n",
    "## summarize total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "## define imputer and fit on data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X)\n",
    "\n",
    "## transform data\n",
    "Ximp = imputer.transform(X)\n",
    "print('Missing: %d' % sum(isnan(Ximp).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer with Model Evaluation\n",
    "- in this example, k-fold cv is used\n",
    "    - to avoid data leakage, the imputer should be applied to each fold (not entire dataset)\n",
    "    - this is done using a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import nan\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "data14 = read_csv(url, header=None, na_values='?')\n",
    "data = data14.values\n",
    "\n",
    "## split data into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.866 (0.061)\n"
     ]
    }
   ],
   "source": [
    "## define model pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "## define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10,\n",
    "                             n_repeats=3,\n",
    "                             random_state=1)\n",
    "\n",
    "## evaluate the model\n",
    "dataModel = cross_val_score(pipeline, X, y,\n",
    "                            scoring='accuracy',\n",
    "                            cv=cv,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(dataModel), std(dataModel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputer: Compare Different Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "data15 = read_csv(url, header=None, na_values='?')\n",
    "\n",
    "data = data15.values\n",
    "\n",
    "## get index for the independent variables\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "## split into dep and indep variables\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">mean 0.861, (0.062)\n",
      ">median 0.873, (0.055)\n",
      ">most_frequent 0.868, (0.051)\n",
      ">constant 0.879, (0.052)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## evaluate model on different imputation strategies\n",
    "results = list()\n",
    "strategies = ['mean', 'median', 'most_frequent', 'constant']\n",
    "\n",
    "for s in strategies:\n",
    "    ## create pipeline\n",
    "    pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)),\n",
    "                               ('m', RandomForestClassifier())])\n",
    "    ## evaluate model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10,\n",
    "                                 n_repeats=3,\n",
    "                                 random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y,\n",
    "                             scoring='accuracy',\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1)\n",
    "    \n",
    "    ## store results using empty list created above\n",
    "    results.append(scores)\n",
    "    ## print results in the loop to get results of each strategy\n",
    "    print('>%s %.3f, (%.3f)' % (s, mean(scores), std(scores)))\n",
    "\n",
    "## plot performance for each strategy\n",
    "pyplot.boxplot(results,\n",
    "               labels=strategies,\n",
    "               showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Imputer: Making a Prediction with Best Strategy\n",
    "- after identifying the best strategy using script above, we re-run the algrithm and predict\n",
    "- Note: still need to use pipeline so imputation can be applied to each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import nan\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we create new data that will be tested once the model is built\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "## load data\n",
    "data16 = read_csv(url, header=None, na_values='?')\n",
    "data = data16.values ## convert to array\n",
    "\n",
    "## get col index for dep vars and split data into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "## create pipeline\n",
    "pipeline = Pipeline(steps=[('i', SimpleImputer(strategy='constant')),\n",
    "                           ('m', RandomForestClassifier())])\n",
    "\n",
    "## fit data\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "## predict using sample observation created above\n",
    "dataPredict = pipeline.predict([row])\n",
    "print('Predicted Class: %d' % dataPredict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Imputation (nearest neighbor imputation)\n",
    "- often involves selecting Euclidean distance and number of contributing neighbors for each prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
      "0  2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
      "1  1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
      "2  2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
      "3  1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
      "4  2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
      "\n",
      "    20   21   22  23     24  25  26  27  \n",
      "0  NaN  NaN  2.0   2  11300   0   0   2  \n",
      "1  2.0  2.0  3.0   2   2208   0   0   2  \n",
      "2  NaN  NaN  1.0   2      0   0   0   1  \n",
      "3  3.0  5.3  2.0   1   2208   0   0   1  \n",
      "4  NaN  NaN  2.0   2   4300   0   0   2  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "data17 = read_csv(url, header=None, na_values='?')\n",
    "print(data17.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 0, Missing: 1 (0.3%)\n",
      "> 1, Missing: 0 (0.0%)\n",
      "> 2, Missing: 0 (0.0%)\n",
      "> 3, Missing: 60 (20.0%)\n",
      "> 4, Missing: 24 (8.0%)\n",
      "> 5, Missing: 58 (19.3%)\n",
      "> 6, Missing: 56 (18.7%)\n",
      "> 7, Missing: 69 (23.0%)\n",
      "> 8, Missing: 47 (15.7%)\n",
      "> 9, Missing: 32 (10.7%)\n",
      "> 10, Missing: 55 (18.3%)\n",
      "> 11, Missing: 44 (14.7%)\n",
      "> 12, Missing: 56 (18.7%)\n",
      "> 13, Missing: 104 (34.7%)\n",
      "> 14, Missing: 106 (35.3%)\n",
      "> 15, Missing: 247 (82.3%)\n",
      "> 16, Missing: 102 (34.0%)\n",
      "> 17, Missing: 118 (39.3%)\n",
      "> 18, Missing: 29 (9.7%)\n",
      "> 19, Missing: 33 (11.0%)\n",
      "> 20, Missing: 165 (55.0%)\n",
      "> 21, Missing: 198 (66.0%)\n",
      "> 22, Missing: 1 (0.3%)\n",
      "> 23, Missing: 0 (0.0%)\n",
      "> 24, Missing: 0 (0.0%)\n",
      "> 25, Missing: 0 (0.0%)\n",
      "> 26, Missing: 0 (0.0%)\n",
      "> 27, Missing: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "## summarize missing data\n",
    "for i in range(data17.shape[1]):\n",
    "    n_miss = data17[[i]].isnull().sum()\n",
    "    perc_miss = n_miss / data17.shape[0] * 100\n",
    "    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc_miss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Imputation with KNNImputer\n",
    "- default distance measurement is Euclidian distance that is NaN aware\n",
    "- number of neighbors default is 5\n",
    "- distance measured can be weighted proportional to distance between rows\n",
    "    - default is uniform weight, but can be controlled via 'weights' argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from numpy import isnan\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "data18 = read_csv(url, header=None, na_values='?')\n",
    "data = data18.values\n",
    "\n",
    "## get column indexes for dep var columns \n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "## split into dep and indep vars\n",
    "X, y = data[:, Xdata], data[:, 23]\n",
    "\n",
    "## summarize missing data in dep vars\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "## define imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "## fit on data and transform values to the data\n",
    "imputer.fit(X)\n",
    "XImp = imputer.transform(X)\n",
    "\n",
    "## summarize missing data after transform\n",
    "print('Missing: %d' % sum(isnan(XImp).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data19 = read_csv(url, header=None, na_values='?')\n",
    "data = data19.values\n",
    "\n",
    "## get dep var col indices and split into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.867, (0.055)\n"
     ]
    }
   ],
   "source": [
    "## define pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = KNNImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "\n",
    "## define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "## evaluate the model\n",
    "dataEval = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=1)\n",
    "print('Mean Accuracy: %.3f, (%.3f)' % (mean(dataEval), std(dataEval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Imputation with KNNImputer different Neighbors\n",
    "- the number of neighbors can impact results so we can test multipe neighbor values\n",
    "- remember default (used above) is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data20 = read_csv(url, header=None, na_values='?')\n",
    "data = data20.values\n",
    "## get column indices and split data into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 0.860 (0.052)\n",
      ">3 0.863 (0.060)\n",
      ">5 0.868 (0.055)\n",
      ">7 0.864 (0.059)\n",
      ">9 0.860 (0.057)\n",
      ">15 0.860 (0.053)\n",
      ">18 0.856 (0.057)\n",
      ">21 0.854 (0.057)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list()\n",
    "strategies = [str(i) for i in [1, 3, 5, 7, 9, 15, 18, 21]]\n",
    "\n",
    "for s in strategies:\n",
    "    ## create pipeline\n",
    "    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))),\n",
    "                               ('m', RandomForestClassifier())])\n",
    "    ## evaluate model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y,\n",
    "                             scoring='accuracy',\n",
    "                             cv=cv,\n",
    "                             n_jobs=-1)\n",
    "    results.append(scores)\n",
    "    print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n",
    "\n",
    "## plot scores for comparison\n",
    "pyplot.boxplot(results, labels=strategies, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNNImputer: Making a Prediction with Best Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define sample data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, \n",
    "       nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, \n",
    "       00000, 00000, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data21 = read_csv(url, header=None, na_values='?')\n",
    "data = data21.values\n",
    "\n",
    "## identify dep var column indices and split into dep and indep vars\n",
    "Xdata = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, Xdata], data[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "## create pipeline\n",
    "## note: from test above, n=5 had best results so we specify that here\n",
    "pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=5)),\n",
    "                           ('m', RandomForestClassifier())])\n",
    "\n",
    "## fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "## predict on new data (see above)\n",
    "dataPred = pipeline.predict([row])\n",
    "\n",
    "## summarize prediciton\n",
    "print('Predicted Class: %d' % dataPred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Imputation (MICE)\n",
    "- aka: Fully Conditional Specification (FCS) or Multivariate Imputation by Chained Equations (MICE)\n",
    "- a model is defined to predict each missing feature as a function of all other features\n",
    "    - this is repeated multiple times\n",
    "    - each feature is imputed sequntially\n",
    "    - repitition allows refined estimates to be used as inputs for subsequent iterations of missing value prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "797px",
    "left": "62px",
    "top": "53px",
    "width": "416.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> e273c708f0679ce5351676df61e954d38e6812a9
