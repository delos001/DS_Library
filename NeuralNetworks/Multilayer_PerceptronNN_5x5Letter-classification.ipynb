{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# We will randomly define initial values for connection weights, and also randomly select\n",
    "#   which training data that we will use for a given run.\n",
    "import random\n",
    "\n",
    "# We want to use the exp function (e to the x); it's part of our transfer function definition\n",
    "from math import exp\n",
    "\n",
    "# Biting the bullet and starting to use NumPy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# So we can make a separate list from an initial one\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pretty-printing the arrays\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure to welcome the user and identify the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welcome ():\n",
    "\n",
    "\n",
    "    print ()\n",
    "    print ('******************************************************************************')\n",
    "    print ()\n",
    "    print ('Welcome to the Multilayer Perceptron Neural Network')\n",
    "    print ('  trained using the backpropagation method.')\n",
    "    print ('Version 0.2, 01/22/2017, A.J. Maren')\n",
    "    print ('For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu')\n",
    "    print (' ' )\n",
    "    print ('This program learns to distinguish between five capital letters: X, M, H, A, and N')\n",
    "    print ('It allows users to examine the hidden weights to identify learned features')\n",
    "    print ()\n",
    "    print ('******************************************************************************')\n",
    "    print ()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A collection of worker-functions, designed to do specific small tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neuron activation using sigmoid transfer function\n",
    "def computeTransferFnctn(summedNeuronInput, alpha):\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summedNeuronInput)) \n",
    "    return activation\n",
    "#------------------------------------------------------# \n",
    "    \n",
    "# Compute derivative of transfer function\n",
    "def computeTransferFnctnDeriv(NeuronOutput, alpha):\n",
    "    return alpha*NeuronOutput*(1.0 -NeuronOutput)     \n",
    "\n",
    "#------------------------------------------------------# \n",
    "def matrixDotProduct (matrx1,matrx2):\n",
    "    dotProduct = np.dot(matrx1,matrx2)\n",
    "    \n",
    "    return(dotProduct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain the neural network size specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainNeuralNetworkSizeSpecs ():\n",
    "\n",
    "    numInputNodes = 25\n",
    "    numHiddenNodes = 6\n",
    "    numOutputNodes = 5   \n",
    "    print (' ')\n",
    "    print ('  The number of nodes at each level are:')\n",
    "    print ('    Input: 5x5 (square array)')\n",
    "    print ('    Hidden: 6')\n",
    "    print ('    Output: 5 (three classes)')\n",
    "            \n",
    "    arraySizeList = (numInputNodes, numHiddenNodes, numOutputNodes)\n",
    "     \n",
    "    return (arraySizeList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeight ():\n",
    "\n",
    "    randomNum = random.random()\n",
    "    weight=1-2*randomNum\n",
    "#    print (weight)\n",
    "           \n",
    "    return (weight)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the node-to-node connection weight arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeightArray (weightArraySizeList):\n",
    "\n",
    "    numBottomNodes = weightArraySizeList[0]\n",
    "    numUpperNodes = weightArraySizeList[1]\n",
    "\n",
    "# Initialize the weight variables with random weights    \n",
    "    weightArray = np.zeros((numUpperNodes,numBottomNodes))    # iniitalize the weight matrix with 0's\n",
    "    for row in range(numUpperNodes):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input matrix (expressed as a column)\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes.\n",
    "        for col in range(numBottomNodes):  # number of columns in matrix 2\n",
    "            weightArray[row,col] = InitializeWeight ()\n",
    "                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (weightArray) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the bias weight arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeBiasWeightArray (numBiasNodes):\n",
    "\n",
    "# This procedure is also called directly from 'main.'\n",
    "\n",
    "# Initialize the bias weight variables with random weights    \n",
    "    biasWeightArray = np.zeros(numBiasNodes)    # iniitalize the weight matrix with 0's\n",
    "    for node in range(numBiasNodes):  #  Number of nodes in bias weight set\n",
    "        biasWeightArray[node] = InitializeWeight ()\n",
    "                  \n",
    "# Print the entire weights array. \n",
    "    print (biasWeightArray)\n",
    "                  \n",
    "    \n",
    "# We return the array to the calling procedure, 'main'.       \n",
    "    return (biasWeightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return a trainingDataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainSelectedAlphabetTrainingValues (trainingDataSetNum):\n",
    "    \n",
    "    if trainingDataSetNum == 0: trainingDataList = (1,0,0,0,1, 0,1,0,1,0, 0,0,1,0,0, 0,1,0,1,0, 1,0,0,0,1, 0,'X')\n",
    "    if trainingDataSetNum == 1: trainingDataList = (1,0,0,0,1, 1,1,0,1,1, 1,0,1,0,1, 1,0,0,0,1, 1,0,0,0,1, 1,'M') \n",
    "    if trainingDataSetNum == 2: trainingDataList = (1,0,0,0,1, 1,1,0,0,1, 1,0,1,0,1, 1,0,0,1,1, 1,0,0,0,1, 2,'N') \n",
    "    if trainingDataSetNum == 3: trainingDataList = (1,0,0,0,1, 1,0,0,0,1, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 3,'H') \n",
    "    if trainingDataSetNum == 4: trainingDataList = (0,0,1,0,0, 0,1,0,1,0, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 4,'A')             \n",
    "                      \n",
    "    return (trainingDataList)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainRandomAlphabetTrainingValues (numTrainingDataSets):\n",
    "\n",
    "   \n",
    "    # The training data list will have 11 values for the X-OR problem:\n",
    "    #   - First nine valuea will be the 5x5 pixel-grid representation of the letter\n",
    "    #       represented as a 1-D array (0 or 1 for each)\n",
    "    #   - Tenth value will be the output class (0 .. totalClasses - 1)\n",
    "    #   - Eleventh value will be the string associated with that class, e.g., 'X'\n",
    "    # We are starting with five letters in the training set: X, M, N, H, and A\n",
    "    # Thus there are five choices for training data, which we'll select on random basis\n",
    "      \n",
    "    trainingDataSetNum = random.randint(0, numTrainingDataSets)\n",
    "    if trainingDataSetNum == 0: trainingDataList = (1,0,0,0,1, 0,1,0,1,0, 0,0,1,0,0, 0,1,0,1,0, 1,0,0,0,1, 0,'X')\n",
    "    if trainingDataSetNum == 1: trainingDataList = (1,0,0,0,1, 1,1,0,1,1, 1,0,1,0,1, 1,0,0,0,1, 1,0,0,0,1, 1,'M') \n",
    "    if trainingDataSetNum == 2: trainingDataList = (1,0,0,0,1, 1,1,0,0,1, 1,0,1,0,1, 1,0,0,1,1, 1,0,0,0,1, 2,'N') \n",
    "    if trainingDataSetNum == 3: trainingDataList = (1,0,0,0,1, 1,0,0,0,1, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 3,'H') \n",
    "    if trainingDataSetNum == 4: trainingDataList = (0,0,1,0,0, 0,1,0,1,0, 1,1,1,1,1, 1,0,0,0,1, 1,0,0,0,1, 4,'A')             \n",
    "                      \n",
    "    return (trainingDataList)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "####################################################################################################\n",
    "#\n",
    "# Perform a single feedforward pass\n",
    "#\n",
    "####################################################################################################\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize a specific connection weight with a randomly-generated number between 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, wWeightArray, \n",
    "biasHiddenWeightArray):\n",
    "\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoHiddenArray = np.zeros(hiddenArrayLength)    \n",
    "    hiddenArray = np.zeros(hiddenArrayLength)   \n",
    "\n",
    "    sumIntoHiddenArray = matrixDotProduct (wWeightArray,inputDataList)\n",
    "    \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        hiddenNodeSumInput=sumIntoHiddenArray[node]+biasHiddenWeightArray[node]\n",
    "        hiddenArray[node] = computeTransferFnctn(hiddenNodeSumInput, alpha)\n",
    "\n",
    "#    print (' ')\n",
    "#    print ('Back in ComputeSingleFeedforwardPass')\n",
    "#    print ('The activations for the hidden nodes are:')\n",
    "#    print ('  Hidden0 = %.4f' % hiddenActivation0, 'Hidden1 = %.4f' % hiddenActivation1)\n",
    "\n",
    "                                                                                                    \n",
    "    return (hiddenArray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the output node activations, given the hidden node activations, the hidden-to\n",
    "#   output connection weights, and the output bias weights.\n",
    "# Function returns the array of output node activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray, vWeightArray, \n",
    "biasOutputWeightArray):\n",
    "\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "    \n",
    "# iniitalize the sum of inputs into the hidden array with 0's  \n",
    "    sumIntoOutputArray = np.zeros(hiddenArrayLength)    \n",
    "    outputArray = np.zeros(outputArrayLength)   \n",
    "\n",
    "    sumIntoOutputArray = matrixDotProduct (vWeightArray,hiddenArray)\n",
    "    \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        outputNodeSumInput=sumIntoOutputArray[node]+biasOutputWeightArray[node]\n",
    "        outputArray[node] = computeTransferFnctn(outputNodeSumInput, alpha)\n",
    "                                                                                                   \n",
    "    return (outputArray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray):\n",
    "\n",
    "    selectedTrainingDataSet = 0\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]                               \n",
    "                              \n",
    "\n",
    "    while selectedTrainingDataSet < numTrainingDataSets + 1: \n",
    "        trainingDataList = obtainSelectedAlphabetTrainingValues (selectedTrainingDataSet)\n",
    "        inputDataList = [] \n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "\n",
    "        print (' ')\n",
    "        print ('  Data Set Number', selectedTrainingDataSet, ' for letter ', trainingDataList[26] )\n",
    "\n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, wWeightArray, biasHiddenWeightArray)\n",
    "\n",
    "        print (' ')\n",
    "        print (' The hidden node activations are:')\n",
    "        print (hiddenArray)\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray, vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "        print (' ')\n",
    "        print (' The output node activations are:')\n",
    "        print (outputArray   )\n",
    "\n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[25]                 # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "        print (' ')\n",
    "        print (' The desired output array values are: ')\n",
    "        print (desiredOutputArray  )\n",
    "       \n",
    "                        \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "        print (' ')\n",
    "        print (' The error values are:')\n",
    "        print (errorArray   )\n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "        print ('New SSE = %.6f' % newSSE )\n",
    "         \n",
    "        selectedTrainingDataSet = selectedTrainingDataSet +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "#   Backpropgation Section\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray):\n",
    "\n",
    "# Unpack array lengths\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "\n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in the equations for the deltas in the connection weights    \n",
    "#    print (' ')\n",
    "#    print (' The transfer function derivative is: ')\n",
    "#    print (transferFuncDerivArray)\n",
    "                        \n",
    "    deltaVWtArray = np.zeros((outputArrayLength, hiddenArrayLength))  # initialize an array for the deltas\n",
    "    newVWeightArray = np.zeros((outputArrayLength, hiddenArrayLength)) # initialize an array for the new hidden weights\n",
    "        \n",
    "    for row in range(outputArrayLength):  #  Number of rows in weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "        # Similarly, for a hidden-to-output matrix, the rows correspond to the number of output nodes,\n",
    "        #    and the columns correspond to the number of hidden nodes,\n",
    "        #    which can be multiplied by the hidden node array (expressed as a column).\n",
    "        for col in range(hiddenArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_V_Wt = -errorArray[row]*transferFuncDerivArray[row]*hiddenArray[col]\n",
    "            deltaVWtArray[row,col] = -eta*partialSSE_w_V_Wt\n",
    "            newVWeightArray[row,col] = vWeightArray[row,col] + deltaVWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print (' ')\n",
    "#    print (' The previous hidden-to-output connection weights are: ')\n",
    "#    print (vWeightArray)\n",
    "#    print (' ')\n",
    "#    print (' The new hidden-to-output connection weights are: ')\n",
    "#    print (newVWeightArray)\n",
    "\n",
    "#    Print (AndTraceBackpropagateOutputToHidden (alpha, nu, errorList, actualAllNodesOutputList, \n",
    "#    transFuncDerivList, deltaVWtArray, vWeightArray, newHiddenWeightArray)    )\n",
    "                                                                  \n",
    "                                                                                                                                                                                                            \n",
    "    return (newVWeightArray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate weight changes onto the bias-to-output connection weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray):\n",
    "\n",
    "# Unpack the output array length\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "    deltaBiasOutputArray = np.zeros(outputArrayLength)  # initialize an array for the deltas\n",
    "    newBiasOutputWeightArray = np.zeros(outputArrayLength) # initialize an array for the new output bias weights\n",
    "    transferFuncDerivArray = np.zeros(outputArrayLength)    # iniitalize an array for the transfer function\n",
    "      \n",
    "    for node in range(outputArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivArray[node]=computeTransferFnctnDeriv(outputArray[node], alpha)\n",
    " \n",
    "\n",
    "    for node in range(outputArrayLength):  #  Number of nodes in output array (same as number of output bias nodes)    \n",
    "        partialSSE_w_BiasOutput = -errorArray[node]*transferFuncDerivArray[node]\n",
    "        deltaBiasOutputArray[node] = -eta*partialSSE_w_BiasOutput  \n",
    "        newBiasOutputWeightArray[node] =  biasOutputWeightArray[node] + deltaBiasOutputArray[node]           \n",
    "   \n",
    "#    print (' ')\n",
    "#    print (' The previous biases for the output nodes are: ')\n",
    "#    print (biasOutputWeightArray)\n",
    "#    print (' ')\n",
    "#    print (' The new biases for the output nodes are: ')\n",
    "#    print (newBiasOutputWeightArray)\n",
    "                                                                                                                                                \n",
    "    return (newBiasOutputWeightArray);     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate weight changes onto the input-to-hidden connection weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations       \n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)    # initialize an array for the transfer function deriv \n",
    "      \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)\n",
    "        \n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array\n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array\n",
    "      \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha)\n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "        \n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = weightedErrorArray[hiddenNode] \\\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode]\n",
    "             \n",
    "    deltaWWtArray = np.zeros((hiddenArrayLength, inputArrayLength))  # initialize an array for the deltas\n",
    "    newWWeightArray = np.zeros((hiddenArrayLength, inputArrayLength)) # initialize an array for the new input-to-hidden weights\n",
    "        \n",
    "    for row in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix\n",
    "        # For an input-to-hidden weight matrix, the rows correspond to the number of hidden nodes,\n",
    "        #    and the columns correspond to the number of input nodes.\n",
    "        #    This creates an HxI matrix, which can be multiplied by the input node array (expressed as a column).\n",
    "\n",
    "        for col in range(inputArrayLength):  # number of columns in weightMatrix\n",
    "            partialSSE_w_W_Wts = -transferFuncDerivHiddenArray[row]*inputArray[col]*weightedErrorArray[row]\n",
    "            deltaWWtArray[row,col] = -eta*partialSSE_w_W_Wts\n",
    "            newWWeightArray[row,col] = wWeightArray[row,col] + deltaWWtArray[row,col]                                                                                     \n",
    "\n",
    "#    print (' ')\n",
    "#    print (' The previous hidden-to-output connection weights are: ')\n",
    "#    print (wWeightArray)\n",
    "#    print (' ')\n",
    "#    print (' The new hidden-to-output connection weights are: ')\n",
    "#    print (newWWeightArray    )\n",
    "       \n",
    "                                                                    \n",
    "    return (newWWeightArray);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate weight changes onto the bias-to-hidden connection weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "    inputArray, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray):\n",
    "\n",
    "# Unpack array lengths\n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]              \n",
    "                                          \n",
    "   \n",
    "# Compute the transfer function derivatives as a function of the output nodes.\n",
    "# Note: As this is being done after the call to the backpropagation on the hidden-to-output weights,\n",
    "#   the transfer function derivative computed there could have been used here; the calculations are\n",
    "#   being redone here only to maintain module independence              \n",
    "\n",
    "    errorTimesTFuncDerivOutputArray = np.zeros(outputArrayLength) # initialize array    \n",
    "    transferFuncDerivOutputArray    = np.zeros(outputArrayLength) # initialize array\n",
    "    weightedErrorArray              = np.zeros(hiddenArrayLength) # initialize array    \n",
    "\n",
    "    transferFuncDerivHiddenArray = np.zeros(hiddenArrayLength)  # initialize an array for the transfer function deriv \n",
    "    partialSSE_w_BiasHidden      = np.zeros(hiddenArrayLength)  # initialize an array for the partial derivative of the SSE\n",
    "    deltaBiasHiddenArray         = np.zeros(hiddenArrayLength)  # initialize an array for the deltas\n",
    "    newBiasHiddenWeightArray     = np.zeros(hiddenArrayLength)  # initialize an array for the new hidden bias weights\n",
    "          \n",
    "    for node in range(hiddenArrayLength):  #  Number of hidden nodes\n",
    "        transferFuncDerivHiddenArray[node]=computeTransferFnctnDeriv(hiddenArray[node], alpha)      \n",
    "                  \n",
    "    for outputNode in range(outputArrayLength):  #  Number of output nodes\n",
    "        transferFuncDerivOutputArray[outputNode]=computeTransferFnctnDeriv(outputArray[outputNode], alpha) \n",
    "        errorTimesTFuncDerivOutputArray[outputNode] = errorArray[outputNode]*transferFuncDerivOutputArray[outputNode]\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):\n",
    "        weightedErrorArray[hiddenNode] = 0\n",
    "        for outputNode in range(outputArrayLength):  #  Number of output nodes    \n",
    "            weightedErrorArray[hiddenNode] = weightedErrorArray[hiddenNode]\n",
    "            + vWeightArray[outputNode, hiddenNode]*errorTimesTFuncDerivOutputArray[outputNode]\n",
    "            \n",
    "# Note: the parameter 'alpha' in the transfer function shows up in the transfer function derivative\n",
    "#   and so is not included explicitly in these equations \n",
    "\n",
    "\n",
    "# ===>>> AJM needs to double-check these equations in the comments area\n",
    "# ===>>> The code should be fine. \n",
    "# The equation for the actual dependence of the Summed Squared Error on a given bias-to-output \n",
    "#   weight biasOutput(o) is:\n",
    "#   partial(SSE)/partial(biasOutput(o)) = -alpha*E(o)*F(o)*[1-F(o)]*1, as '1' is the input from the bias.\n",
    "# The transfer function derivative (transFuncDeriv) returned from computeTransferFnctnDeriv is given as:\n",
    "#   transFuncDeriv =  alpha*NeuronOutput*(1.0 -NeuronOutput), as with the hidden-to-output weights.\n",
    "# Therefore, we can write the equation for the partial(SSE)/partial(biasOutput(o)) as\n",
    "#   partial(SSE)/partial(biasOutput(o)) = E(o)*transFuncDeriv\n",
    "#   The parameter alpha is included in transFuncDeriv\n",
    "\n",
    "    for hiddenNode in range(hiddenArrayLength):  #  Number of rows in input-to-hidden weightMatrix           \n",
    "        partialSSE_w_BiasHidden[hiddenNode] = -transferFuncDerivHiddenArray[hiddenNode]*weightedErrorArray[hiddenNode]\n",
    "        deltaBiasHiddenArray[hiddenNode] = -eta*partialSSE_w_BiasHidden[hiddenNode]\n",
    "        newBiasHiddenWeightArray[hiddenNode] = biasHiddenWeightArray[hiddenNode] + deltaBiasHiddenArray[hiddenNode]                                                                                                                                                                                                                                                         \n",
    "  \n",
    "                                                                                                                                            \n",
    "    return (newBiasHiddenWeightArray); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "# The MAIN module comprising of calls to:\n",
    "#   (1) Welcome\n",
    "#   (2) Obtain neural network size specifications for a three-layer network consisting of:\n",
    "#       - Input layer\n",
    "#       - Hidden layer\n",
    "#       - Output layer (all the sizes are currently hard-coded to two nodes per layer right now)\n",
    "#   (3) Initialize connection weight values\n",
    "#       - w: Input-to-Hidden nodes\n",
    "#       - v: Hidden-to-Output nodes\n",
    "#   (4) Compute a feedforward pass in two steps\n",
    "#       - Randomly select a single training data set\n",
    "#       - Input-to-Hidden\n",
    "#       - Hidden-to-Output\n",
    "#       - Compute the error array\n",
    "#       - Compute the new Summed Squared Error (SSE)\n",
    "#   (5) Perform a single backpropagation training pass\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "\n",
      "Welcome to the Multilayer Perceptron Neural Network\n",
      "  trained using the backpropagation method.\n",
      "Version 0.2, 01/22/2017, A.J. Maren\n",
      "For comments, questions, or bug-fixes, contact: alianna.maren@northwestern.edu\n",
      " \n",
      "This program learns to distinguish between five capital letters: X, M, H, A, and N\n",
      "It allows users to examine the hidden weights to identify learned features\n",
      "\n",
      "******************************************************************************\n",
      "\n",
      " \n",
      "  The number of nodes at each level are:\n",
      "    Input: 5x5 (square array)\n",
      "    Hidden: 6\n",
      "    Output: 5 (three classes)\n",
      "[ 0.421 -0.262  0.77  -0.073  0.143 -0.952]\n",
      "[ 0.147  0.446  0.194 -0.857  0.818]\n",
      " \n",
      "  Before training:\n",
      " \n",
      "  Data Set Number 0  for letter  X\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.233 0.834 0.487 0.824 0.282 0.049]\n",
      " \n",
      " The output node activations are:\n",
      "[0.633 0.459 0.293 0.304 0.748]\n",
      " \n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[ 0.367 -0.459 -0.293 -0.304 -0.748]\n",
      "New SSE = 1.083360\n",
      " \n",
      "  Data Set Number 1  for letter  M\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.464 0.748 0.421 0.703 0.193 0.175]\n",
      " \n",
      " The output node activations are:\n",
      "[0.589 0.417 0.281 0.322 0.683]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.589  0.583 -0.281 -0.322 -0.683]\n",
      "New SSE = 1.335210\n",
      " \n",
      "  Data Set Number 2  for letter  N\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.218 0.678 0.521 0.639 0.425 0.385]\n",
      " \n",
      " The output node activations are:\n",
      "[0.651 0.394 0.294 0.287 0.694]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 1. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.651 -0.394  0.706 -0.287 -0.694]\n",
      "New SSE = 1.642473\n",
      " \n",
      "  Data Set Number 3  for letter  H\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.212 0.507 0.18  0.448 0.248 0.18 ]\n",
      " \n",
      " The output node activations are:\n",
      "[0.57  0.468 0.388 0.271 0.723]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.57  -0.468 -0.388  0.729 -0.723]\n",
      "New SSE = 1.749271\n",
      " \n",
      "  Data Set Number 4  for letter  A\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.166 0.381 0.807 0.775 0.919 0.011]\n",
      " \n",
      " The output node activations are:\n",
      "[0.667 0.487 0.349 0.403 0.772]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1.]\n",
      " \n",
      " The error values are:\n",
      "[-0.667 -0.487 -0.349 -0.403  0.228]\n",
      "New SSE = 1.018707\n",
      "Out of while loop at iteration  536\n",
      " \n",
      "  After training:\n",
      " \n",
      "  Data Set Number 0  for letter  X\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.015 0.978 0.911 0.869 0.077 0.315]\n",
      " \n",
      " The output node activations are:\n",
      "[0.866 0.161 0.087 0.015 0.07 ]\n",
      " \n",
      " The desired output array values are: \n",
      "[1. 0. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[ 0.134 -0.161 -0.087 -0.015 -0.07 ]\n",
      "New SSE = 0.056368\n",
      " \n",
      "  Data Set Number 1  for letter  M\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.677 0.855 0.026 0.172 0.115 0.145]\n",
      " \n",
      " The output node activations are:\n",
      "[0.098 0.596 0.133 0.204 0.048]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 1. 0. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.098  0.404 -0.133 -0.204 -0.048]\n",
      "New SSE = 0.234390\n",
      " \n",
      "  Data Set Number 2  for letter  N\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.241 0.65  0.044 0.055 0.492 0.872]\n",
      " \n",
      " The output node activations are:\n",
      "[0.098 0.139 0.684 0.097 0.082]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 1. 0. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.098 -0.139  0.316 -0.097 -0.082]\n",
      "New SSE = 0.144993\n",
      " \n",
      "  Data Set Number 3  for letter  H\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.939 0.097 0.006 0.059 0.477 0.276]\n",
      " \n",
      " The output node activations are:\n",
      "[0.014 0.277 0.183 0.718 0.153]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 1. 0.]\n",
      " \n",
      " The error values are:\n",
      "[-0.014 -0.277 -0.183  0.282 -0.153]\n",
      "New SSE = 0.213596\n",
      " \n",
      "  Data Set Number 4  for letter  A\n",
      " \n",
      " The hidden node activations are:\n",
      "[0.288 0.24  0.066 0.911 0.982 0.006]\n",
      " \n",
      " The output node activations are:\n",
      "[0.096 0.071 0.073 0.107 0.869]\n",
      " \n",
      " The desired output array values are: \n",
      "[0. 0. 0. 0. 1.]\n",
      " \n",
      " The error values are:\n",
      "[-0.096 -0.071 -0.073 -0.107  0.131]\n",
      "New SSE = 0.048234\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "#\n",
    "# The MAIN module comprising of calls to:\n",
    "#   (1) Welcome\n",
    "#   (2) Obtain neural network size specifications for a three-layer network consisting of:\n",
    "#       - Input layer\n",
    "#       - Hidden layer\n",
    "#       - Output layer (all the sizes are currently hard-coded to two nodes per layer right now)\n",
    "#   (3) Initialize connection weight values\n",
    "#       - w: Input-to-Hidden nodes\n",
    "#       - v: Hidden-to-Output nodes\n",
    "#   (4) Compute a feedforward pass in two steps\n",
    "#       - Randomly select a single training data set\n",
    "#       - Input-to-Hidden\n",
    "#       - Hidden-to-Output\n",
    "#       - Compute the error array\n",
    "#       - Compute the new Summed Squared Error (SSE)\n",
    "#   (5) Perform a single backpropagation training pass\n",
    "#\n",
    "####################################################################################################\n",
    "#**************************************************************************************************#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "####################################################################################################\n",
    "# Obtain unit array size in terms of array_length (M) and layers (N)\n",
    "####################################################################################################                \n",
    "\n",
    "# This calls the procedure 'welcome,' which just prints out a welcoming message. \n",
    "# All procedures need an argument list. \n",
    "# This procedure has a list, but it is an empty list; welcome().\n",
    "\n",
    "    welcome()\n",
    "    \n",
    "\n",
    "# Parameter definitions, to be replaced with user inputs\n",
    "    alpha = 1.0\n",
    "    eta = 0.5    \n",
    "    maxNumIterations = 5000    # temporarily set to 10 for testing\n",
    "    epsilon = 0.05\n",
    "    iteration = 0\n",
    "    SSE = 0.0\n",
    "    numTrainingDataSets = 4\n",
    "    \n",
    "# Right now, for simplicity, we're going to hard-code the numbers of layers that we have in our \n",
    "#   multilayer Perceptron (MLP) neural network. \n",
    "# We will have an input layer (I), an output layer (O), and a single hidden layer (H). \n",
    "\n",
    "# Define the variable arraySizeList, which is a list. It is initially an empty list. \n",
    "# Its purpose is to store the size of the array.\n",
    "\n",
    "    arraySizeList = list() # empty list\n",
    "\n",
    "# Obtain the actual sizes for each layer of the network       \n",
    "    arraySizeList = obtainNeuralNetworkSizeSpecs ()\n",
    "    \n",
    "# Unpack the list; ascribe the various elements of the list to the sizes of different network layers    \n",
    "    inputArrayLength = arraySizeList [0]\n",
    "    hiddenArrayLength = arraySizeList [1]\n",
    "    outputArrayLength = arraySizeList [2]\n",
    "\n",
    "                           \n",
    "\n",
    "####################################################################################################\n",
    "# Initialize the weight arrays for two sets of weights; w: input-to-hidden, and v: hidden-to-output\n",
    "####################################################################################################                \n",
    "\n",
    "#\n",
    "# The wWeightArray is for Input-to-Hidden\n",
    "# The vWeightArray is for Hidden-to-Output\n",
    "\n",
    "    wWeightArraySizeList = (inputArrayLength, hiddenArrayLength)\n",
    "    vWeightArraySizeList = (hiddenArrayLength, outputArrayLength)\n",
    "    biasHiddenWeightArraySize = hiddenArrayLength\n",
    "    biasOutputWeightArraySize = outputArrayLength        \n",
    "\n",
    "# The node-to-node connection weights are stored in a 2-D array\n",
    "    wWeightArray = initializeWeightArray (wWeightArraySizeList)\n",
    "    vWeightArray = initializeWeightArray (vWeightArraySizeList)\n",
    "\n",
    "# The bias weights are stored in a 1-D array         \n",
    "    biasHiddenWeightArray = initializeBiasWeightArray (biasHiddenWeightArraySize)\n",
    "    biasOutputWeightArray = initializeBiasWeightArray (biasOutputWeightArraySize) \n",
    "\n",
    "\n",
    "# Notice in the very beginning of the program, we have \n",
    "#   np.set_printoptions(precision=4) (sets number of dec. places in print)\n",
    "#     and 'np.set_printoptions(suppress=True)', which keeps it from printing in scientific format\n",
    "#   Debug print: \n",
    "#    print ()\n",
    "#    print ('The initial weights for this neural network are:')\n",
    "#    print ('       Input-to-Hidden ')\n",
    "#    print (wWeightArray)\n",
    "#    print ('       Hidden-to-Output')\n",
    "#    print (vWeightArray)\n",
    "#    print (' ')\n",
    "#    print ('The initial bias weights for this neural network are:')\n",
    "#    print ('        Hidden Bias = ', biasHiddenWeightArray  )                       \n",
    "#    print ('        Output Bias = ', biasOutputWeightArray)\n",
    "  \n",
    "\n",
    "          \n",
    "####################################################################################################\n",
    "# Before we start training, get a baseline set of outputs, errors, and SSE \n",
    "####################################################################################################                \n",
    "           \n",
    "    trainingDataList = (0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0, ' ')                  \n",
    "    print (' ')\n",
    "    print ('  Before training:')\n",
    "    \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray)                           \n",
    "                                             \n",
    "          \n",
    "####################################################################################################\n",
    "# Next step - Obtain a single set of randomly-selected training values for alpha-classification \n",
    "####################################################################################################                \n",
    "  \n",
    "  \n",
    "    while iteration < maxNumIterations:           \n",
    "\n",
    "# Increment the iteration count\n",
    "        iteration = iteration +1\n",
    "    \n",
    "# For any given pass, we re-initialize the training list\n",
    "        trainingDataList = (0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,0, 0, ' ')\n",
    "\n",
    "        inputDataList = []                                \n",
    "                                                                                          \n",
    "# Randomly select one of four training sets; the inputs will be randomly assigned to 0 or 1\n",
    "        trainingDataList = obtainRandomAlphabetTrainingValues (numTrainingDataSets) \n",
    "\n",
    "\n",
    "        for node in range(inputArrayLength): \n",
    "            trainingData = trainingDataList[node]  \n",
    "            inputDataList.append(trainingData)\n",
    "        \n",
    "#        print (' ')\n",
    "#        print ('  The input data list for letter', trainingDataList[26], ' is:')\n",
    "#        print (inputDataList[0], inputDataList[1], inputDataList[2], inputDataList[3], inputDataList[4])\n",
    "#        print (inputDataList[5], inputDataList[6], inputDataList[7], inputDataList[8], inputDataList[9]   )      \n",
    "#        print (inputDataList[10], inputDataList[11], inputDataList[12], inputDataList[13], inputDataList[14])\n",
    "#        print (inputDataList[15], inputDataList[16], inputDataList[17], inputDataList[18], inputDataList[19]  )      \n",
    "#        print (inputDataList[20], inputDataList[21], inputDataList[22], inputDataList[23], inputDataList[24])\n",
    "#        print (' ')\n",
    "    \n",
    "\n",
    "        desiredOutputArray = np.zeros(outputArrayLength)    # iniitalize the output array with 0's\n",
    "        desiredClass = trainingDataList[25]                 # identify the desired class\n",
    "        desiredOutputArray[desiredClass] = 1                # set the desired output for that class to 1\n",
    "     \n",
    "#        print (' ')\n",
    "#        print (' The desired output array values are: ')\n",
    "#        print (desiredOutputArray  )\n",
    "#        print (' ')\n",
    "\n",
    "         \n",
    "          \n",
    "####################################################################################################\n",
    "# Compute a single feed-forward pass and obtain the Actual Outputs\n",
    "####################################################################################################                \n",
    "                \n",
    "                           \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, \n",
    "        wWeightArray, biasHiddenWeightArray)\n",
    "    \n",
    "#        print (' ')\n",
    "#        print (' The hidden node activations are:')\n",
    "#        print (hiddenArray)\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray,\n",
    "        vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "#        print (' ')\n",
    "#        print (' The output node activations are:')\n",
    " #       print (outputArray    )\n",
    "\n",
    "#  Optional alternative code for later use:\n",
    "#  Assign the hidden and output values to specific different variables\n",
    "#    for node in range(hiddenArrayLength):    \n",
    "#        actualHiddenOutput[node] = actualAllNodesOutputList [node]\n",
    "    \n",
    "#    for node in range(outputArrayLength):    \n",
    "#        actualOutput[node] = actualAllNodesOutputList [hiddenArrayLength + node]\n",
    " \n",
    "# Initialize the error array\n",
    "        errorArray = np.zeros(outputArrayLength) \n",
    "    \n",
    "# Determine the error between actual and desired outputs        \n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "#        print (' ')\n",
    "#        print (' The error values are:')\n",
    "#        print (errorArray   )\n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "#        print ('Initial SSE = %.6f' % newSSE)\n",
    "#        SSE = newSSE\n",
    "\n",
    "         \n",
    "          \n",
    "####################################################################################################\n",
    "# Perform backpropagation\n",
    "####################################################################################################                \n",
    "                \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes    \n",
    "        newVWeightArray = BackpropagateOutputToHidden (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray, vWeightArray)\n",
    "        newBiasOutputWeightArray = BackpropagateBiasOutputWeights (alpha, eta, arraySizeList, errorArray, outputArray, biasOutputWeightArray) \n",
    "\n",
    "# Perform first part of the backpropagation of weight changes       \n",
    "        newWWeightArray = BackpropagateHiddenToInput (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)\n",
    "\n",
    "        newBiasHiddenWeightArray = BackpropagateBiasHiddenWeights (alpha, eta, arraySizeList, errorArray, outputArray, hiddenArray,\n",
    "        inputDataList, vWeightArray, wWeightArray, biasHiddenWeightArray, biasOutputWeightArray)  \n",
    "    \n",
    "                    \n",
    "# Assign new values to the weight matrices\n",
    "# Assign the old hidden-to-output weight array to be the same as what was returned from the BP weight update\n",
    "        vWeightArray = newVWeightArray[:]\n",
    "    \n",
    "        biasOutputWeightArray = newBiasOutputWeightArray[:]\n",
    "    \n",
    "# Assign the old input-to-hidden weight array to be the same as what was returned from the BP weight update\n",
    "        wWeightArray = newWWeightArray[:]  \n",
    "    \n",
    "        biasHiddenWeightArray = newBiasHiddenWeightArray[:] \n",
    "    \n",
    "# Compute a forward pass, test the new SSE                                                                                \n",
    "                                                                                                                                    \n",
    "        hiddenArray = ComputeSingleFeedforwardPassFirstStep (alpha, arraySizeList, inputDataList, \n",
    "        wWeightArray, biasHiddenWeightArray)\n",
    "    \n",
    "#    print (' ')\n",
    "#    print (' The hidden node activations are:')\n",
    "#    print (hiddenArray)\n",
    "\n",
    "        outputArray = ComputeSingleFeedforwardPassSecondStep (alpha, arraySizeList, hiddenArray,\n",
    "        vWeightArray, biasOutputWeightArray)\n",
    "    \n",
    "#    print (' ')\n",
    "#    print (' The output node activations are:')\n",
    "#    print (outputArray    )\n",
    "\n",
    "    \n",
    "# Determine the error between actual and desired outputs\n",
    "\n",
    "        newSSE = 0.0\n",
    "        for node in range(outputArrayLength):  #  Number of nodes in output set (classes)\n",
    "            errorArray[node] = desiredOutputArray[node] - outputArray[node]\n",
    "            newSSE = newSSE + errorArray[node]*errorArray[node]        \n",
    "\n",
    "#        print (' ')\n",
    "#        print (' The error values are:')\n",
    "#        print (errorArray   )\n",
    "        \n",
    "# Print the Summed Squared Error  \n",
    "#        print ('Previous SSE = %.6f' % SSE)\n",
    "#        print ('New SSE = %.6f' % newSSE )\n",
    "    \n",
    "#        print (' ')\n",
    "#        print ('Iteration number ', iteration)\n",
    "#        iteration = iteration + 1\n",
    "\n",
    "        if newSSE < epsilon:\n",
    "\n",
    "            \n",
    "            break\n",
    "    print ('Out of while loop at iteration ', iteration )\n",
    "    \n",
    "####################################################################################################\n",
    "# After training, get a new comparative set of outputs, errors, and SSE \n",
    "####################################################################################################                           \n",
    "\n",
    "    print (' ')\n",
    "    print ('  After training:' )                 \n",
    "                                                      \n",
    "    ComputeOutputsAcrossAllTrainingData (alpha, arraySizeList, numTrainingDataSets, wWeightArray, \n",
    "biasHiddenWeightArray, vWeightArray, biasOutputWeightArray) \n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "                                              \n",
    "####################################################################################################\n",
    "# Conclude specification of the MAIN procedure\n",
    "####################################################################################################                \n",
    "    \n",
    "if __name__ == \"__main__\": main()\n",
    "\n",
    "####################################################################################################\n",
    "# End program\n",
    "#################################################################################################### \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3)\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]]\n",
      "\n",
      " [[11 12 13]\n",
      "  [14 15 16]\n",
      "  [17 18 19]]\n",
      "\n",
      " [[21 22 23]\n",
      "  [24 25 26]\n",
      "  [27 28 29]]]\n",
      "(3, 3, 3)\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]]\n",
      "\n",
      " [[11 12 13]\n",
      "  [14 15 16]\n",
      "  [17 18 19]]\n",
      "\n",
      " [[21 22 23]\n",
      "  [24 25 26]\n",
      "  [27 28 29]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create tensor\n",
    "from numpy import array\n",
    "T = array([\n",
    "  [[1,2,3],    [4,5,6],    [7,8,9]],\n",
    "  [[11,12,13], [14,15,16], [17,18,19]],\n",
    "  [[21,22,23], [24,25,26], [27,28,29]],\n",
    "  ])\n",
    "print(T.shape)\n",
    "print(T)\n",
    "\n",
    "\n",
    "# create tensor\n",
    "from numpy import array\n",
    "T = array([\n",
    "  [[1,2,3],    [4,5,6],    [7,8,9]],\n",
    "  [[11,12,13], [14,15,16], [17,18,19]],\n",
    "  [[21,22,23], [24,25,26], [27,28,29]],\n",
    "  ])\n",
    "print(T.shape)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
